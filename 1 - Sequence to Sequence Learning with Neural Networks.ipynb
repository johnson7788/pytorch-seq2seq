{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Sequence to Sequence Learning with Neural Networks\n",
    "\n",
    "使用PyTorch和TorchText 实现seq2seq ，德语到英语的翻译，也可以适用于信息提取，文字摘要等\n",
    "\n",
    "依据论文地址https://arxiv.org/abs/1409.3215\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Encoder阶段\n",
    "seq2seq是encoder-decoder结构， encoder部分是RNN结构，接收输入的文本向量\n",
    "c 是conctext vector,上下文的向量就是encoder后的输出, decoder部分也是一个RNN结构\n",
    "\n",
    "![](assets/seq2seq1.png)\n",
    "上图示例，输入\"guten morgen\"德语，翻译成英语\n",
    "这里用<SOS>表示句子起始，<EOS>表示结尾，也可以用其它的代替，xt是当前的词的向量，$h_{t-1}$是上一时刻的输出的隐藏状态，ht是表示当前RNN输出的隐藏状态\n",
    "\n",
    "$$h_t = \\text{EncoderRNN}(x_t, h_{t-1})$$\n",
    "\n",
    "RNN的cell可以是LSTM或这GRU，论文中是GRU\n",
    "输入X可以表示成$X = \\{x_1, x_2, ..., x_T\\}$， x1是SOS,x2是第一个单词的向量，encoder部分的初始隐藏状态h0是全零向量\n",
    "\n",
    "这里的z应该是c才对，c = tanh(Vht)，ht是encoder的输出\n",
    "\n",
    "### Decoder阶段\n",
    "这里有点问题，yt是预测的时候的时候的输出，这里的h0应该也是用tanh(V'c)生成的，就是第一个s0\n",
    "ht 是由yt，ht-1，c计算得到，如下公式少了c\n",
    "$$s_t = \\text{DecoderRNN}(y_t, s_{t-1})$$\n",
    "\n",
    "我们用得到的st经过maxout和softmax后得到预测的单词的最大概率\n",
    "\n",
    "$$\\hat{y}_t = f(s_t)$$\n",
    "我们通常使用sos作为decoder阶段的第一个单词输入\n",
    "训练阶段，我们使用真实值与预测值的差值作为decoder的输出损失计算，在测试阶段，持续生成单词直到生成eos为止\n",
    "\n",
    "## Preparing Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim    #优化器\n",
    "\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "# import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#随机种子\n",
    "SEED = 1234  \n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizers, 句子分成列表 \n",
    "e.g. \"good morning!\" becomes [\"good\", \"morning\", \"!\"]\n",
    "使用spacy模块进行tokenizers\n",
    "```\n",
    "python -m spacy download en\n",
    "python -m spacy download de\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spacy_de 和 spacy_en是tokenizer的func，我们把句子传给它，就会返回一个tokens列表\n",
    "反转输入单词的顺序模型效果更好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
    "    [::-1] 德语需要逆序输出\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how', 'are', 'you', '!']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_de(\"guten morgen\")\n",
    "tokenize_en('how are you !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TorchText Field 函数处理数据， 德语作为SRC， 英语作为TRG， 所有单词全部小写，并且附加SOS 和EOS在句子开始末尾等处"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize = tokenize_de, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载训练集，测试集和验证集\n",
    "The dataset we'll be using is the [Multi30k dataset](https://github.com/multi30k/dataset). This is a dataset with ~30,000 parallel English, German and French sentences, each with ~12 words per sentence. \n",
    "\n",
    "`exts` specifies which languages to use as the source and target (source goes first) and `fields` specifies which field to use for the source and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
    "                                                    fields = (SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印一个样本，src句子已经反转了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': ['.', 'büsche', 'vieler', 'nähe', 'der', 'in', 'freien', 'im', 'sind', 'männer', 'weiße', 'junge', 'zwei'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给encoder和decoder的语言建立单词表，vocabulary， 用于单词到索引和索引到单词， 并且建立one-hot编码\n",
    "使用最小频率法min_freq， 至少出现过2次的单词才能放入到字典中，否则用unk作为token\n",
    "字典使用训练集建立，不能用验证集和测试集建立"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (de) vocabulary: 7855\n",
      "Unique tokens in target (en) vocabulary: 5893\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")\n",
    "# SRC.vocab.stoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预处理数据的最后一步是创建一个迭代器，这个迭代器返回一个bacth的数据\n",
    "定义torch.device， 运行在GPU上还是CPU，使用torch.cuda.is_available()自动判断是否存在GPU\n",
    "当获取到一个batch的样本时，需要确保所有的SRC的长度相同， 所有的TRG的长度也相同\n",
    "BucketIterator 就是创建一个长度都一样的桶的迭代器返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Seq2Seq Model\n",
    "创建encoder和decoder\n",
    "\n",
    "### Encoder\n",
    "encoder是2层的LSTM，论文上是4层，2层会更节约训练时间\n",
    "ht1 是第一层的LSTM，ht2是第二层的\n",
    "$$h_t^1 = \\text{EncoderRNN}^1(x_t, h_{t-1}^1)$$\n",
    "$$h_t^2 = \\text{EncoderRNN}^2(h_t^1, h_{t-1}^2)$$\n",
    "我们需要给每一层都初始化一个h0\n",
    "\n",
    "标准RNN和LSTM的输出\n",
    "$$\\begin{align*}\n",
    "h_t &= \\text{RNN}(x_t, h_{t-1})\\\\\n",
    "(h_t, c_t) &= \\text{LSTM}(x_t, (h_{t-1}, c_{t-1}))\n",
    "\\end{align*}$$\n",
    "\n",
    "第一层和第二层的c0和h0都是全零的tensor\n",
    "第一层和第二层都是LSTM的结构，输入和输出如下 \n",
    "$$\\begin{align*}\n",
    "(h_t^1, c_t^1) &= \\text{EncoderLSTM}^1(x_t, (h_{t-1}^1, c_{t-1}^1))\\\\\n",
    "(h_t^2, c_t^2) &= \\text{EncoderLSTM}^2(h_t^1, (h_{t-1}^2, c_{t-1}^2))\n",
    "\\end{align*}$$\n",
    "\n",
    "encoder结构如下\n",
    "![](assets/seq2seq2.png)\n",
    "\n",
    "通过继承torch.nn.Module，自定义一个Encoder类, Encoder类接收如下参数:\n",
    "- `input_dim` 输入维度数，等于字典的大小，因为是用的one-hot编码\n",
    "- `emb_dim` embedding layer 维度，词嵌入的维度\n",
    "- `hid_dim` is the dimensionality of the hidden and cell states.就是一个cell的里面的维度\n",
    "- `n_layers` RNN的层数\n",
    "- `dropout` dropout参数\n",
    "\n",
    "在`forward` method中，传入德语作为X，然后被转换成密集向量通过词嵌入，然后做dropout（这时候做？）， 然后传入RNN，如果不指定cell或者hidden的state，可以不用初始化h0或c0\n",
    "\n",
    "RNN 返回 outputs 是最顶层的layer的隐藏状态， hidden 是每层的隐藏状态， cell是每层的cell state\n",
    "这里只需要 hidden and cell states\n",
    "n_directions 是RNN的方向，这里是1， 当用双向RNN时，bidirectional RNNs 会变成2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch中LSTM公式\n",
    "\\begin{array}{ll} \\\\\n",
    " i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n",
    " f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\\n",
    " o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\\n",
    " g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{(t-1)} + b_{hg}) \\\\\n",
    " c_t = f_t * c_{(t-1)} + i_t * g_t \\\\\n",
    " h_t = o_t * \\tanh(c_t) \\\\\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        # dropout重复\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "Decoder也是2层LSTM\n",
    "![](assets/seq2seq3.png)\n",
    "\n",
    "解码阶段，第2层的LSTM使用第一层的隐藏状态ht，下图下的是st，和st-1，和ct-1\n",
    "$$\\begin{align*}\n",
    "(s_t^1, c_t^1) = \\text{DecoderLSTM}^1(y_t, (s_{t-1}^1, c_{t-1}^1))\\\\\n",
    "(s_t^2, c_t^2) = \\text{DecoderLSTM}^2(s_t^1, (s_{t-1}^2, c_{t-1}^2))\n",
    "\\end{align*}$$\n",
    "\n",
    "初始化s0和c0，这里有些问题，和seq2seq公式不对应\n",
    "$(s_0^l,c_0^l)=z^l=(h_T^l,c_T^l)$.\n",
    "\n",
    "这里直接用linear layer求输出yhat，与seq2seq公式不相符\n",
    "$$\\hat{y}_{t+1} = f(s_t^L)$$\n",
    "\n",
    "RNN输出的output经过Linear layer后预测\n",
    "\n",
    "**Note**: as we always have a sequence length of 1, we could use `nn.LSTMCell`, instead of `nn.LSTM`, as it is designed to handle a batch of inputs that aren't necessarily in a sequence. `nn.LSTMCell` is just a single cell and `nn.LSTM` is a wrapper around potentially multiple cells. Using the `nn.LSTMCell` in this case would mean we don't have to `unsqueeze` to add a fake sequence length dimension, but we would need one `nn.LSTMCell` per layer in the decoder and to ensure each `nn.LSTMCell` receives the correct initial hidden state from the encoder. All of this makes the code less concise - hence the decision to stick with the regular `nn.LSTM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        \n",
    "        #input = [batch size]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #context = [n layers, batch size, hid dim]\n",
    "        #添加一个维度，一个单词一个单词的预测\n",
    "        input = input.unsqueeze(0)  \n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "                \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq\n",
    "把encoder和decoder穿起来\n",
    "![](assets/seq2seq4.png)\n",
    "我门需要让encoder和decoder的layers数相同，隐藏层的维度相同，也可以不同，手动转换一下中间维度即可\n",
    "\n",
    "迭代流程\n",
    "- 传入input, previous hidden and previous cell states ($y_t, s_{t-1}, c_{t-1}$) into the decoder\n",
    "- receive a prediction, next hidden state and next cell state ($\\hat{y}_{t+1}, s_{t}, c_{t}$) from the decoder\n",
    "- place our prediction, $\\hat{y}_{t+1}$/`output` in our tensor of predictions, $\\hat{Y}$/`outputs`\n",
    "- decide if we are going to \"teacher force\" or not\n",
    "    - if we do, the next `input` is the ground-truth next token in the sequence, $y_{t+1}$/`trg[t]`\n",
    "    - if we don't, the next `input` is the predicted next token in the sequence, $\\hat{y}_{t+1}$/`top1`, which we get by doing an `argmax` over the output tensor\n",
    "    \n",
    "Once we've made all of our predictions, we return our tensor full of predictions, $\\hat{Y}$/`outputs`.\n",
    "\n",
    "**Note**: our decoder loop starts at 1, not 0. This means the 0th element of our `outputs` tensor remains all zeros. So our `trg` and `outputs` look something like:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{trg} = [<sos>, &y_1, y_2, y_3, <eos>]\\\\\n",
    "\\text{outputs} = [0, &\\hat{y}_1, \\hat{y}_2, \\hat{y}_3, <eos>]\n",
    "\\end{align*}$$\n",
    "\n",
    "Later on when we calculate the loss, we cut off the first element of each tensor to get:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{trg} = [&y_1, y_2, y_3, <eos>]\\\\\n",
    "\\text{outputs} = [&\\hat{y}_1, \\hat{y}_2, \\hat{y}_3, <eos>]\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio 是使用真实值作为下一个输入的概率\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一步就是初始化参数，初始化方法是均匀分布uniform distribution between -0.08 and +0.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7855, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(5893, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=512, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义模型要训练的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 13,899,013 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用adam优化器 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数使用CrossEntropyLoss\n",
    "忽略计算<pad>的词语的损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]    #<pad>的索引是1\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测值和真实值类似如下\n",
    "$$\\begin{align*}\n",
    "\\text{trg} = [<sos>, &y_1, y_2, y_3, <eos>]\\\\\n",
    "\\text{outputs} = [0, &\\hat{y}_1, \\hat{y}_2, \\hat{y}_3, <eos>]\n",
    "\\end{align*}$$\n",
    "\n",
    "去掉第一个不需要的计算损失的位置后\n",
    "$$\\begin{align*}\n",
    "\\text{trg} = [&y_1, y_2, y_3, <eos>]\\\\\n",
    "\\text{outputs} = [&\\hat{y}_1, \\hat{y}_2, \\hat{y}_3, <eos>]\n",
    "\\end{align*}$$\n",
    "\n",
    "每次迭代\n",
    "- 获取 $X$ and $Y$ 的批次数据\n",
    "- zero the gradients calculated from the last batch\n",
    "- 喂入数据x和y,得到预测值 $\\hat{Y}$\n",
    "- flatten 真实值和预测值，为了计算损失\n",
    "- 计算梯度 `loss.backward()`\n",
    "- 梯度截断，防止梯度爆炸\n",
    "- 更新模型参数通过优化器\n",
    "- 计算总的损失值\n",
    "\n",
    "最后，返回所有batch的损失的和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        print('The %s batch loss is %s' % (i, loss.item()))\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试阶段evaluation mode \n",
    "使用with torch.no_grad() 不更新梯度\n",
    "model.eval() 不用做dropout\n",
    "关闭teacher forcing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "        \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算一个epoch所消耗的时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练模型，如果验证集效果损失达到最好，停止训练并保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 0 batch loss is 5.227309703826904\n",
      "The 1 batch loss is 5.18184757232666\n",
      "The 2 batch loss is 5.241634368896484\n",
      "The 3 batch loss is 5.3097453117370605\n",
      "The 4 batch loss is 5.301510810852051\n",
      "The 5 batch loss is 5.311967372894287\n",
      "The 6 batch loss is 5.176915645599365\n",
      "The 7 batch loss is 5.19198751449585\n",
      "The 8 batch loss is 5.212375640869141\n",
      "The 9 batch loss is 5.267341613769531\n",
      "The 10 batch loss is 5.166488170623779\n",
      "The 11 batch loss is 5.163708209991455\n",
      "The 12 batch loss is 5.062734127044678\n",
      "The 13 batch loss is 5.11533784866333\n",
      "The 14 batch loss is 5.219797134399414\n",
      "The 15 batch loss is 5.204813003540039\n",
      "The 16 batch loss is 5.164453983306885\n",
      "The 17 batch loss is 5.141345977783203\n",
      "The 18 batch loss is 5.200850009918213\n",
      "The 19 batch loss is 5.052821159362793\n",
      "The 20 batch loss is 5.056247711181641\n",
      "The 21 batch loss is 5.130373954772949\n",
      "The 22 batch loss is 5.2228803634643555\n",
      "The 23 batch loss is 5.107409477233887\n",
      "The 24 batch loss is 5.150378704071045\n",
      "The 25 batch loss is 5.028877258300781\n",
      "The 26 batch loss is 5.163486480712891\n",
      "The 27 batch loss is 5.080452919006348\n",
      "The 28 batch loss is 5.108501434326172\n",
      "The 29 batch loss is 5.073175430297852\n",
      "The 30 batch loss is 5.153021335601807\n",
      "The 31 batch loss is 5.117766380310059\n",
      "The 32 batch loss is 5.0409016609191895\n",
      "The 33 batch loss is 5.056113243103027\n",
      "The 34 batch loss is 5.031601428985596\n",
      "The 35 batch loss is 5.024582862854004\n",
      "The 36 batch loss is 5.054033279418945\n",
      "The 37 batch loss is 5.192097187042236\n",
      "The 38 batch loss is 5.099915504455566\n",
      "The 39 batch loss is 5.114957809448242\n",
      "The 40 batch loss is 4.930144786834717\n",
      "The 41 batch loss is 5.088237762451172\n",
      "The 42 batch loss is 5.04546594619751\n",
      "The 43 batch loss is 5.059759616851807\n",
      "The 44 batch loss is 4.924698352813721\n",
      "The 45 batch loss is 5.146091461181641\n",
      "The 46 batch loss is 5.064854621887207\n",
      "The 47 batch loss is 5.179187297821045\n",
      "The 48 batch loss is 5.08860445022583\n",
      "The 49 batch loss is 5.092214107513428\n",
      "The 50 batch loss is 5.019208908081055\n",
      "The 51 batch loss is 5.01657772064209\n",
      "The 52 batch loss is 5.108035087585449\n",
      "The 53 batch loss is 5.030945301055908\n",
      "The 54 batch loss is 4.980749607086182\n",
      "The 55 batch loss is 5.036587238311768\n",
      "The 56 batch loss is 5.064029693603516\n",
      "The 57 batch loss is 4.957708835601807\n",
      "The 58 batch loss is 5.019964694976807\n",
      "The 59 batch loss is 5.105549335479736\n",
      "The 60 batch loss is 5.095420837402344\n",
      "The 61 batch loss is 5.144714832305908\n",
      "The 62 batch loss is 4.948999881744385\n",
      "The 63 batch loss is 5.10087776184082\n",
      "The 64 batch loss is 5.027127265930176\n",
      "The 65 batch loss is 5.0300984382629395\n",
      "The 66 batch loss is 4.891218662261963\n",
      "The 67 batch loss is 4.914701461791992\n",
      "The 68 batch loss is 5.110921382904053\n",
      "The 69 batch loss is 5.033120155334473\n",
      "The 70 batch loss is 5.064518928527832\n",
      "The 71 batch loss is 5.0790252685546875\n",
      "The 72 batch loss is 5.001562595367432\n",
      "The 73 batch loss is 4.920919418334961\n",
      "The 74 batch loss is 5.084628582000732\n",
      "The 75 batch loss is 4.9785966873168945\n",
      "The 76 batch loss is 4.931454658508301\n",
      "The 77 batch loss is 4.98383903503418\n",
      "The 78 batch loss is 5.02787446975708\n",
      "The 79 batch loss is 4.888307571411133\n",
      "The 80 batch loss is 5.0238165855407715\n",
      "The 81 batch loss is 5.078222751617432\n",
      "The 82 batch loss is 5.065191268920898\n",
      "The 83 batch loss is 4.9394354820251465\n",
      "The 84 batch loss is 4.98242712020874\n",
      "The 85 batch loss is 5.032087802886963\n",
      "The 86 batch loss is 5.015377521514893\n",
      "The 87 batch loss is 5.00753927230835\n",
      "The 88 batch loss is 5.012231826782227\n",
      "The 89 batch loss is 4.967705249786377\n",
      "The 90 batch loss is 4.960517406463623\n",
      "The 91 batch loss is 5.056751728057861\n",
      "The 92 batch loss is 4.997913837432861\n",
      "The 93 batch loss is 5.013415336608887\n",
      "The 94 batch loss is 4.847219467163086\n",
      "The 95 batch loss is 4.992136001586914\n",
      "The 96 batch loss is 4.928831100463867\n",
      "The 97 batch loss is 4.983757495880127\n",
      "The 98 batch loss is 4.9780778884887695\n",
      "The 99 batch loss is 4.925625801086426\n",
      "The 100 batch loss is 4.964339733123779\n",
      "The 101 batch loss is 4.811196327209473\n",
      "The 102 batch loss is 4.9625396728515625\n",
      "The 103 batch loss is 4.960690975189209\n",
      "The 104 batch loss is 4.985289573669434\n",
      "The 105 batch loss is 4.905927658081055\n",
      "The 106 batch loss is 5.052386283874512\n",
      "The 107 batch loss is 4.877997398376465\n",
      "The 108 batch loss is 5.034316062927246\n",
      "The 109 batch loss is 4.890717029571533\n",
      "The 110 batch loss is 4.9153361320495605\n",
      "The 111 batch loss is 4.955818176269531\n",
      "The 112 batch loss is 5.040881156921387\n",
      "The 113 batch loss is 4.832808017730713\n",
      "The 114 batch loss is 4.938439846038818\n",
      "The 115 batch loss is 4.9227752685546875\n",
      "The 116 batch loss is 4.874850273132324\n",
      "The 117 batch loss is 4.809196949005127\n",
      "The 118 batch loss is 4.944483280181885\n",
      "The 119 batch loss is 4.960507392883301\n",
      "The 120 batch loss is 4.966763019561768\n",
      "The 121 batch loss is 4.977489948272705\n",
      "The 122 batch loss is 4.824454307556152\n",
      "The 123 batch loss is 4.880683422088623\n",
      "The 124 batch loss is 5.058987617492676\n",
      "The 125 batch loss is 4.97374153137207\n",
      "The 126 batch loss is 4.960322856903076\n",
      "The 127 batch loss is 4.966085910797119\n",
      "The 128 batch loss is 4.91773796081543\n",
      "The 129 batch loss is 4.904726028442383\n",
      "The 130 batch loss is 5.004810333251953\n",
      "The 131 batch loss is 4.829934120178223\n",
      "The 132 batch loss is 4.827202796936035\n",
      "The 133 batch loss is 4.810769081115723\n",
      "The 134 batch loss is 4.834601879119873\n",
      "The 135 batch loss is 4.81232213973999\n",
      "The 136 batch loss is 5.006611347198486\n",
      "The 137 batch loss is 4.807449817657471\n",
      "The 138 batch loss is 4.780718803405762\n",
      "The 139 batch loss is 4.95299768447876\n",
      "The 140 batch loss is 4.727470397949219\n",
      "The 141 batch loss is 4.830840587615967\n",
      "The 142 batch loss is 4.872003078460693\n",
      "The 143 batch loss is 4.802576065063477\n",
      "The 144 batch loss is 4.928986072540283\n",
      "The 145 batch loss is 4.993711948394775\n",
      "The 146 batch loss is 4.803961277008057\n",
      "The 147 batch loss is 4.951517105102539\n",
      "The 148 batch loss is 4.863287925720215\n",
      "The 149 batch loss is 4.936032295227051\n",
      "The 150 batch loss is 4.841949462890625\n",
      "The 151 batch loss is 4.912319183349609\n",
      "The 152 batch loss is 4.815876007080078\n",
      "The 153 batch loss is 4.849189758300781\n",
      "The 154 batch loss is 4.8830366134643555\n",
      "The 155 batch loss is 4.727093696594238\n",
      "The 156 batch loss is 4.873154163360596\n",
      "The 157 batch loss is 4.831111431121826\n",
      "The 158 batch loss is 4.794970989227295\n",
      "The 159 batch loss is 4.844207763671875\n",
      "The 160 batch loss is 4.891592025756836\n",
      "The 161 batch loss is 4.8492512702941895\n",
      "The 162 batch loss is 4.675093173980713\n",
      "The 163 batch loss is 4.904265880584717\n",
      "The 164 batch loss is 4.823143005371094\n",
      "The 165 batch loss is 4.681426525115967\n",
      "The 166 batch loss is 4.830952167510986\n",
      "The 167 batch loss is 4.8410162925720215\n",
      "The 168 batch loss is 4.871694087982178\n",
      "The 169 batch loss is 4.8570451736450195\n",
      "The 170 batch loss is 4.729160308837891\n",
      "The 171 batch loss is 4.658219814300537\n",
      "The 172 batch loss is 4.762254238128662\n",
      "The 173 batch loss is 4.84383487701416\n",
      "The 174 batch loss is 4.616114139556885\n",
      "The 175 batch loss is 4.808404445648193\n",
      "The 176 batch loss is 4.689589977264404\n",
      "The 177 batch loss is 4.727606296539307\n",
      "The 178 batch loss is 4.819086074829102\n",
      "The 179 batch loss is 4.77793550491333\n",
      "The 180 batch loss is 4.614261627197266\n",
      "The 181 batch loss is 4.769715309143066\n",
      "The 182 batch loss is 4.700613021850586\n",
      "The 183 batch loss is 4.56337308883667\n",
      "The 184 batch loss is 4.898160934448242\n",
      "The 185 batch loss is 4.873773574829102\n",
      "The 186 batch loss is 4.73243522644043\n",
      "The 187 batch loss is 4.879222393035889\n",
      "The 188 batch loss is 4.767088890075684\n",
      "The 189 batch loss is 4.650125026702881\n",
      "The 190 batch loss is 4.648461818695068\n",
      "The 191 batch loss is 4.617824554443359\n",
      "The 192 batch loss is 4.666830539703369\n",
      "The 193 batch loss is 4.732814311981201\n",
      "The 194 batch loss is 4.933319091796875\n",
      "The 195 batch loss is 4.784236431121826\n",
      "The 196 batch loss is 4.86161470413208\n",
      "The 197 batch loss is 4.608123302459717\n",
      "The 198 batch loss is 4.570748805999756\n",
      "The 199 batch loss is 4.529963493347168\n",
      "The 200 batch loss is 4.821302890777588\n",
      "The 201 batch loss is 4.718315601348877\n",
      "The 202 batch loss is 4.6398773193359375\n",
      "The 203 batch loss is 4.787857532501221\n",
      "The 204 batch loss is 4.691190242767334\n",
      "The 205 batch loss is 4.598549842834473\n",
      "The 206 batch loss is 4.653552055358887\n",
      "The 207 batch loss is 4.833871364593506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 208 batch loss is 4.624127388000488\n",
      "The 209 batch loss is 4.8941802978515625\n",
      "The 210 batch loss is 4.716525077819824\n",
      "The 211 batch loss is 4.693122386932373\n",
      "The 212 batch loss is 4.802249431610107\n",
      "The 213 batch loss is 4.684479713439941\n",
      "The 214 batch loss is 4.8478617668151855\n",
      "The 215 batch loss is 4.644590854644775\n",
      "The 216 batch loss is 4.612752437591553\n",
      "The 217 batch loss is 4.643129825592041\n",
      "The 218 batch loss is 4.735301494598389\n",
      "The 219 batch loss is 4.7895188331604\n",
      "The 220 batch loss is 4.69575834274292\n",
      "The 221 batch loss is 4.765822887420654\n",
      "The 222 batch loss is 4.633568286895752\n",
      "The 223 batch loss is 4.453860759735107\n",
      "The 224 batch loss is 4.858541965484619\n",
      "The 225 batch loss is 4.764775276184082\n",
      "The 226 batch loss is 4.652652263641357\n",
      "Epoch: 01 | Time: 28m 52s\n",
      "\tTrain Loss: 4.925 | Train PPL: 137.757\n",
      "\t Val. Loss: 4.960 |  Val. PPL: 142.572\n",
      "The 0 batch loss is 4.797477722167969\n",
      "The 1 batch loss is 4.631030559539795\n",
      "The 2 batch loss is 4.597670555114746\n",
      "The 3 batch loss is 4.479500770568848\n",
      "The 4 batch loss is 4.636916160583496\n",
      "The 5 batch loss is 4.692566394805908\n",
      "The 6 batch loss is 4.468676567077637\n",
      "The 7 batch loss is 4.7598137855529785\n",
      "The 8 batch loss is 4.8336944580078125\n",
      "The 9 batch loss is 4.650454521179199\n",
      "The 10 batch loss is 4.707118034362793\n",
      "The 11 batch loss is 4.67930793762207\n",
      "The 12 batch loss is 4.629251956939697\n",
      "The 13 batch loss is 4.81596040725708\n",
      "The 14 batch loss is 4.759481906890869\n",
      "The 15 batch loss is 4.478850841522217\n",
      "The 16 batch loss is 4.562100887298584\n",
      "The 17 batch loss is 4.711692810058594\n",
      "The 18 batch loss is 4.551015853881836\n",
      "The 19 batch loss is 4.665256500244141\n",
      "The 20 batch loss is 4.539323806762695\n",
      "The 21 batch loss is 4.583446979522705\n",
      "The 22 batch loss is 4.754316329956055\n",
      "The 23 batch loss is 4.607569694519043\n",
      "The 24 batch loss is 4.592131614685059\n",
      "The 25 batch loss is 4.549784183502197\n",
      "The 26 batch loss is 4.6083879470825195\n",
      "The 27 batch loss is 4.586948871612549\n",
      "The 28 batch loss is 4.497859954833984\n",
      "The 29 batch loss is 4.678692817687988\n",
      "The 30 batch loss is 4.5402021408081055\n",
      "The 31 batch loss is 4.621819496154785\n",
      "The 32 batch loss is 4.771442890167236\n",
      "The 33 batch loss is 4.599503040313721\n",
      "The 34 batch loss is 4.557071208953857\n",
      "The 35 batch loss is 4.675835132598877\n",
      "The 36 batch loss is 4.471631050109863\n",
      "The 37 batch loss is 4.764777183532715\n",
      "The 38 batch loss is 4.691158294677734\n",
      "The 39 batch loss is 4.672389507293701\n",
      "The 40 batch loss is 4.601456165313721\n",
      "The 41 batch loss is 4.769642353057861\n",
      "The 42 batch loss is 4.51537561416626\n",
      "The 43 batch loss is 4.6808013916015625\n",
      "The 44 batch loss is 4.605339527130127\n",
      "The 45 batch loss is 4.593432903289795\n",
      "The 46 batch loss is 4.581701755523682\n",
      "The 47 batch loss is 4.571364402770996\n",
      "The 48 batch loss is 4.551422595977783\n",
      "The 49 batch loss is 4.44126558303833\n",
      "The 50 batch loss is 4.687594890594482\n",
      "The 51 batch loss is 4.594894886016846\n",
      "The 52 batch loss is 4.30369234085083\n",
      "The 53 batch loss is 4.646388530731201\n",
      "The 54 batch loss is 4.418496131896973\n",
      "The 55 batch loss is 4.659097671508789\n",
      "The 56 batch loss is 4.666558742523193\n",
      "The 57 batch loss is 4.5742645263671875\n",
      "The 58 batch loss is 4.709182262420654\n",
      "The 59 batch loss is 4.408748626708984\n",
      "The 60 batch loss is 4.50992488861084\n",
      "The 61 batch loss is 4.56248664855957\n",
      "The 62 batch loss is 4.609131336212158\n",
      "The 63 batch loss is 4.807238578796387\n",
      "The 64 batch loss is 4.385328769683838\n",
      "The 65 batch loss is 4.571375370025635\n",
      "The 66 batch loss is 4.538430213928223\n",
      "The 67 batch loss is 4.280703544616699\n",
      "The 68 batch loss is 4.781876564025879\n",
      "The 69 batch loss is 4.4337944984436035\n",
      "The 70 batch loss is 4.280184268951416\n",
      "The 71 batch loss is 4.754861354827881\n",
      "The 72 batch loss is 4.310752868652344\n",
      "The 73 batch loss is 4.601998329162598\n",
      "The 74 batch loss is 4.502960205078125\n",
      "The 75 batch loss is 4.788249492645264\n",
      "The 76 batch loss is 4.646840572357178\n",
      "The 77 batch loss is 4.403836250305176\n",
      "The 78 batch loss is 4.551018238067627\n",
      "The 79 batch loss is 4.659466743469238\n",
      "The 80 batch loss is 4.544370174407959\n",
      "The 81 batch loss is 4.5075507164001465\n",
      "The 82 batch loss is 4.425027370452881\n",
      "The 83 batch loss is 4.603677272796631\n",
      "The 84 batch loss is 4.4521660804748535\n",
      "The 85 batch loss is 4.38569450378418\n",
      "The 86 batch loss is 4.492973327636719\n",
      "The 87 batch loss is 4.551841735839844\n",
      "The 88 batch loss is 4.609067440032959\n",
      "The 89 batch loss is 4.693922519683838\n",
      "The 90 batch loss is 4.704156398773193\n",
      "The 91 batch loss is 4.585373401641846\n",
      "The 92 batch loss is 4.31581974029541\n",
      "The 93 batch loss is 4.685997486114502\n",
      "The 94 batch loss is 4.464353084564209\n",
      "The 95 batch loss is 4.686755180358887\n",
      "The 96 batch loss is 4.592101097106934\n",
      "The 97 batch loss is 4.403994083404541\n",
      "The 98 batch loss is 4.534269332885742\n",
      "The 99 batch loss is 4.669905185699463\n",
      "The 100 batch loss is 4.482973098754883\n",
      "The 101 batch loss is 4.722226619720459\n",
      "The 102 batch loss is 4.358304500579834\n",
      "The 103 batch loss is 4.541032791137695\n",
      "The 104 batch loss is 4.440639972686768\n",
      "The 105 batch loss is 4.710500717163086\n",
      "The 106 batch loss is 4.629980087280273\n",
      "The 107 batch loss is 4.406259536743164\n",
      "The 108 batch loss is 4.161158561706543\n",
      "The 109 batch loss is 4.468178749084473\n",
      "The 110 batch loss is 4.4861159324646\n",
      "The 111 batch loss is 4.406588554382324\n",
      "The 112 batch loss is 4.730098247528076\n",
      "The 113 batch loss is 4.401744842529297\n",
      "The 114 batch loss is 4.292287349700928\n",
      "The 115 batch loss is 4.883437633514404\n",
      "The 116 batch loss is 4.884109973907471\n",
      "The 117 batch loss is 4.557910442352295\n",
      "The 118 batch loss is 4.658262729644775\n",
      "The 119 batch loss is 4.430366039276123\n",
      "The 120 batch loss is 4.611088275909424\n",
      "The 121 batch loss is 4.360896110534668\n",
      "The 122 batch loss is 4.856037616729736\n",
      "The 123 batch loss is 4.3310346603393555\n",
      "The 124 batch loss is 4.425052165985107\n",
      "The 125 batch loss is 4.589808940887451\n",
      "The 126 batch loss is 4.658304214477539\n",
      "The 127 batch loss is 4.483614921569824\n",
      "The 128 batch loss is 4.4109272956848145\n",
      "The 129 batch loss is 4.65611457824707\n",
      "The 130 batch loss is 4.585599422454834\n",
      "The 131 batch loss is 4.429109573364258\n",
      "The 132 batch loss is 4.410171985626221\n",
      "The 133 batch loss is 4.4361677169799805\n",
      "The 134 batch loss is 4.533827781677246\n",
      "The 135 batch loss is 4.515944480895996\n",
      "The 136 batch loss is 4.488709926605225\n",
      "The 137 batch loss is 4.586087703704834\n",
      "The 138 batch loss is 4.409128189086914\n",
      "The 139 batch loss is 4.464509963989258\n",
      "The 140 batch loss is 4.695481777191162\n",
      "The 141 batch loss is 4.436330795288086\n",
      "The 142 batch loss is 4.477901935577393\n",
      "The 143 batch loss is 4.424559116363525\n",
      "The 144 batch loss is 4.563520908355713\n",
      "The 145 batch loss is 4.554889678955078\n",
      "The 146 batch loss is 4.884964466094971\n",
      "The 147 batch loss is 4.649765491485596\n",
      "The 148 batch loss is 4.629099369049072\n",
      "The 149 batch loss is 4.382845878601074\n",
      "The 150 batch loss is 4.600412845611572\n",
      "The 151 batch loss is 4.487541198730469\n",
      "The 152 batch loss is 4.458972930908203\n",
      "The 153 batch loss is 4.1532182693481445\n",
      "The 154 batch loss is 4.453649997711182\n",
      "The 155 batch loss is 4.479555130004883\n",
      "The 156 batch loss is 4.469982624053955\n",
      "The 157 batch loss is 4.597984313964844\n",
      "The 158 batch loss is 4.561973571777344\n",
      "The 159 batch loss is 4.477646350860596\n",
      "The 160 batch loss is 4.428587913513184\n",
      "The 161 batch loss is 4.27099609375\n",
      "The 162 batch loss is 4.458022594451904\n",
      "The 163 batch loss is 4.514524459838867\n",
      "The 164 batch loss is 4.576912879943848\n",
      "The 165 batch loss is 4.523916721343994\n",
      "The 166 batch loss is 4.22323751449585\n",
      "The 167 batch loss is 4.754930019378662\n",
      "The 168 batch loss is 4.692424297332764\n",
      "The 169 batch loss is 4.36392068862915\n",
      "The 170 batch loss is 4.762608528137207\n",
      "The 171 batch loss is 4.499014377593994\n",
      "The 172 batch loss is 4.309916973114014\n",
      "The 173 batch loss is 4.330695152282715\n",
      "The 174 batch loss is 4.137318134307861\n",
      "The 175 batch loss is 4.555989742279053\n",
      "The 176 batch loss is 4.415879726409912\n",
      "The 177 batch loss is 4.580147743225098\n",
      "The 178 batch loss is 4.50074577331543\n",
      "The 179 batch loss is 4.26054573059082\n",
      "The 180 batch loss is 4.428328514099121\n",
      "The 181 batch loss is 4.333643436431885\n",
      "The 182 batch loss is 4.725625514984131\n",
      "The 183 batch loss is 4.502852916717529\n",
      "The 184 batch loss is 5.006827354431152\n",
      "The 185 batch loss is 4.477428436279297\n",
      "The 186 batch loss is 4.472973823547363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 187 batch loss is 4.38511848449707\n",
      "The 188 batch loss is 4.5495829582214355\n",
      "The 189 batch loss is 4.470181465148926\n",
      "The 190 batch loss is 4.445215225219727\n",
      "The 191 batch loss is 4.3130292892456055\n",
      "The 192 batch loss is 4.513171195983887\n",
      "The 193 batch loss is 4.377730846405029\n",
      "The 194 batch loss is 4.733048439025879\n",
      "The 195 batch loss is 4.615927696228027\n",
      "The 196 batch loss is 4.302975177764893\n",
      "The 197 batch loss is 4.529049396514893\n",
      "The 198 batch loss is 4.659888744354248\n",
      "The 199 batch loss is 4.368391990661621\n",
      "The 200 batch loss is 4.438083171844482\n",
      "The 201 batch loss is 4.471482276916504\n",
      "The 202 batch loss is 4.4749650955200195\n",
      "The 203 batch loss is 4.561156749725342\n",
      "The 204 batch loss is 4.47731876373291\n",
      "The 205 batch loss is 4.310968399047852\n",
      "The 206 batch loss is 4.344976425170898\n",
      "The 207 batch loss is 4.321959972381592\n",
      "The 208 batch loss is 4.3902812004089355\n",
      "The 209 batch loss is 4.406135559082031\n",
      "The 210 batch loss is 4.488180637359619\n",
      "The 211 batch loss is 4.425560474395752\n",
      "The 212 batch loss is 4.42235803604126\n",
      "The 213 batch loss is 4.5953240394592285\n",
      "The 214 batch loss is 4.522213459014893\n",
      "The 215 batch loss is 4.403123378753662\n",
      "The 216 batch loss is 4.6295552253723145\n",
      "The 217 batch loss is 4.473292350769043\n",
      "The 218 batch loss is 4.368309497833252\n",
      "The 219 batch loss is 4.245558261871338\n",
      "The 220 batch loss is 4.2082438468933105\n",
      "The 221 batch loss is 4.281439304351807\n",
      "The 222 batch loss is 4.481910228729248\n",
      "The 223 batch loss is 4.315822601318359\n",
      "The 224 batch loss is 4.383295059204102\n",
      "The 225 batch loss is 4.5881547927856445\n",
      "The 226 batch loss is 4.410033702850342\n",
      "Epoch: 02 | Time: 24m 55s\n",
      "\tTrain Loss: 4.533 | Train PPL:  93.077\n",
      "\t Val. Loss: 4.924 |  Val. PPL: 137.494\n",
      "The 0 batch loss is 4.436495304107666\n",
      "The 1 batch loss is 4.324789524078369\n",
      "The 2 batch loss is 4.270618915557861\n",
      "The 3 batch loss is 4.632361888885498\n",
      "The 4 batch loss is 4.363204479217529\n",
      "The 5 batch loss is 4.121925354003906\n",
      "The 6 batch loss is 4.344032287597656\n",
      "The 7 batch loss is 4.158015727996826\n",
      "The 8 batch loss is 4.260500431060791\n",
      "The 9 batch loss is 4.534571170806885\n",
      "The 10 batch loss is 4.216423988342285\n",
      "The 11 batch loss is 4.28273344039917\n",
      "The 12 batch loss is 4.3938822746276855\n",
      "The 13 batch loss is 4.229086399078369\n",
      "The 14 batch loss is 4.178221702575684\n",
      "The 15 batch loss is 4.252328872680664\n",
      "The 16 batch loss is 4.469971656799316\n",
      "The 17 batch loss is 4.570589542388916\n",
      "The 18 batch loss is 4.5445733070373535\n",
      "The 19 batch loss is 4.238941192626953\n",
      "The 20 batch loss is 4.206093788146973\n",
      "The 21 batch loss is 4.111150741577148\n",
      "The 22 batch loss is 4.207759380340576\n",
      "The 23 batch loss is 4.8722453117370605\n",
      "The 24 batch loss is 4.5195817947387695\n",
      "The 25 batch loss is 4.208280086517334\n",
      "The 26 batch loss is 4.306924343109131\n",
      "The 27 batch loss is 4.378870010375977\n",
      "The 28 batch loss is 4.599060535430908\n",
      "The 29 batch loss is 4.3593430519104\n",
      "The 30 batch loss is 4.505644798278809\n",
      "The 31 batch loss is 4.381947994232178\n",
      "The 32 batch loss is 4.362855434417725\n",
      "The 33 batch loss is 4.7486982345581055\n",
      "The 34 batch loss is 4.2584309577941895\n",
      "The 35 batch loss is 4.237381935119629\n",
      "The 36 batch loss is 4.34462833404541\n",
      "The 37 batch loss is 4.311601638793945\n",
      "The 38 batch loss is 4.453774929046631\n",
      "The 39 batch loss is 4.253417015075684\n",
      "The 40 batch loss is 4.353115081787109\n",
      "The 41 batch loss is 4.116987705230713\n",
      "The 42 batch loss is 4.468816757202148\n",
      "The 43 batch loss is 4.233589172363281\n",
      "The 44 batch loss is 4.407958507537842\n",
      "The 45 batch loss is 4.569083213806152\n",
      "The 46 batch loss is 4.557190418243408\n",
      "The 47 batch loss is 4.128137588500977\n",
      "The 48 batch loss is 4.410082817077637\n",
      "The 49 batch loss is 4.283680438995361\n",
      "The 50 batch loss is 4.632937908172607\n",
      "The 51 batch loss is 4.4285197257995605\n",
      "The 52 batch loss is 4.254000186920166\n",
      "The 53 batch loss is 4.440633773803711\n",
      "The 54 batch loss is 4.470524311065674\n",
      "The 55 batch loss is 4.095152854919434\n",
      "The 56 batch loss is 4.055344104766846\n",
      "The 57 batch loss is 4.0157999992370605\n",
      "The 58 batch loss is 4.177320957183838\n",
      "The 59 batch loss is 4.0675554275512695\n",
      "The 60 batch loss is 4.3021159172058105\n",
      "The 61 batch loss is 4.472365856170654\n",
      "The 62 batch loss is 4.532926082611084\n",
      "The 63 batch loss is 4.222613334655762\n",
      "The 64 batch loss is 4.723177433013916\n",
      "The 65 batch loss is 4.150105953216553\n",
      "The 66 batch loss is 4.317749977111816\n",
      "The 67 batch loss is 4.236240863800049\n",
      "The 68 batch loss is 4.420013427734375\n",
      "The 69 batch loss is 4.275656223297119\n",
      "The 70 batch loss is 4.201919078826904\n",
      "The 71 batch loss is 4.289177417755127\n",
      "The 72 batch loss is 4.2019572257995605\n",
      "The 73 batch loss is 4.299639701843262\n",
      "The 74 batch loss is 4.7470173835754395\n",
      "The 75 batch loss is 4.21557092666626\n",
      "The 76 batch loss is 4.251218795776367\n",
      "The 77 batch loss is 4.371434688568115\n",
      "The 78 batch loss is 4.086509704589844\n",
      "The 79 batch loss is 4.130374431610107\n",
      "The 80 batch loss is 4.195450782775879\n",
      "The 81 batch loss is 4.2220611572265625\n",
      "The 82 batch loss is 4.418592929840088\n",
      "The 83 batch loss is 4.392731189727783\n",
      "The 84 batch loss is 4.388925075531006\n",
      "The 85 batch loss is 4.4083991050720215\n",
      "The 86 batch loss is 4.249328136444092\n",
      "The 87 batch loss is 4.259067535400391\n",
      "The 88 batch loss is 4.377233505249023\n",
      "The 89 batch loss is 4.362349987030029\n",
      "The 90 batch loss is 4.274904727935791\n",
      "The 91 batch loss is 4.262123107910156\n",
      "The 92 batch loss is 4.434968948364258\n",
      "The 93 batch loss is 4.7110490798950195\n",
      "The 94 batch loss is 4.455092906951904\n",
      "The 95 batch loss is 4.4750823974609375\n",
      "The 96 batch loss is 4.030895709991455\n",
      "The 97 batch loss is 4.122596263885498\n",
      "The 98 batch loss is 4.240634918212891\n",
      "The 99 batch loss is 4.1831374168396\n",
      "The 100 batch loss is 4.225029945373535\n",
      "The 101 batch loss is 4.584522247314453\n",
      "The 102 batch loss is 4.427162170410156\n",
      "The 103 batch loss is 4.103434085845947\n",
      "The 104 batch loss is 4.448792457580566\n",
      "The 105 batch loss is 4.11462926864624\n",
      "The 106 batch loss is 4.510160446166992\n",
      "The 107 batch loss is 4.352071285247803\n",
      "The 108 batch loss is 4.250816822052002\n",
      "The 109 batch loss is 4.262049674987793\n",
      "The 110 batch loss is 4.354869365692139\n",
      "The 111 batch loss is 4.212318420410156\n",
      "The 112 batch loss is 4.262954235076904\n",
      "The 113 batch loss is 4.3062944412231445\n",
      "The 114 batch loss is 4.335247039794922\n",
      "The 115 batch loss is 4.399969577789307\n",
      "The 116 batch loss is 4.462753772735596\n",
      "The 117 batch loss is 4.315927982330322\n",
      "The 118 batch loss is 4.450470924377441\n",
      "The 119 batch loss is 4.138393878936768\n",
      "The 120 batch loss is 4.292247295379639\n",
      "The 121 batch loss is 4.008248805999756\n",
      "The 122 batch loss is 4.436507225036621\n",
      "The 123 batch loss is 4.4412150382995605\n",
      "The 124 batch loss is 4.175940036773682\n",
      "The 125 batch loss is 4.07558012008667\n",
      "The 126 batch loss is 4.149348735809326\n",
      "The 127 batch loss is 4.131624698638916\n",
      "The 128 batch loss is 4.2896904945373535\n",
      "The 129 batch loss is 4.111497402191162\n",
      "The 130 batch loss is 4.146399974822998\n",
      "The 131 batch loss is 4.32382345199585\n",
      "The 132 batch loss is 4.217445373535156\n",
      "The 133 batch loss is 4.626145362854004\n",
      "The 134 batch loss is 4.332918167114258\n",
      "The 135 batch loss is 4.159506797790527\n",
      "The 136 batch loss is 4.108456611633301\n",
      "The 137 batch loss is 4.2485270500183105\n",
      "The 138 batch loss is 4.080963611602783\n",
      "The 139 batch loss is 4.241209983825684\n",
      "The 140 batch loss is 4.481626033782959\n",
      "The 141 batch loss is 4.061659336090088\n",
      "The 142 batch loss is 4.13934850692749\n",
      "The 143 batch loss is 4.294900417327881\n",
      "The 144 batch loss is 4.396359920501709\n",
      "The 145 batch loss is 4.211406707763672\n",
      "The 146 batch loss is 4.289505481719971\n",
      "The 147 batch loss is 4.115488529205322\n",
      "The 148 batch loss is 4.164026737213135\n",
      "The 149 batch loss is 4.202089309692383\n",
      "The 150 batch loss is 4.14760684967041\n",
      "The 151 batch loss is 4.440877437591553\n",
      "The 152 batch loss is 4.1737213134765625\n",
      "The 153 batch loss is 4.163875102996826\n",
      "The 154 batch loss is 4.362758159637451\n",
      "The 155 batch loss is 4.349159240722656\n",
      "The 156 batch loss is 4.357424259185791\n",
      "The 157 batch loss is 4.329801559448242\n",
      "The 158 batch loss is 4.3832550048828125\n",
      "The 159 batch loss is 4.084738254547119\n",
      "The 160 batch loss is 4.405124187469482\n",
      "The 161 batch loss is 4.123602390289307\n",
      "The 162 batch loss is 4.217737197875977\n",
      "The 163 batch loss is 4.2097039222717285\n",
      "The 164 batch loss is 4.349493503570557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 165 batch loss is 4.410851955413818\n",
      "The 166 batch loss is 4.172938346862793\n",
      "The 167 batch loss is 4.141534805297852\n",
      "The 168 batch loss is 4.1402106285095215\n",
      "The 169 batch loss is 4.24867057800293\n",
      "The 170 batch loss is 4.18686056137085\n",
      "The 171 batch loss is 4.1696600914001465\n",
      "The 172 batch loss is 4.417567729949951\n",
      "The 173 batch loss is 4.372066020965576\n",
      "The 174 batch loss is 4.341028690338135\n",
      "The 175 batch loss is 4.31783390045166\n",
      "The 176 batch loss is 4.0787739753723145\n",
      "The 177 batch loss is 4.081729888916016\n",
      "The 178 batch loss is 4.504328727722168\n",
      "The 179 batch loss is 4.191431999206543\n",
      "The 180 batch loss is 3.9646105766296387\n",
      "The 181 batch loss is 4.264227390289307\n",
      "The 182 batch loss is 4.145939350128174\n",
      "The 183 batch loss is 4.2639594078063965\n",
      "The 184 batch loss is 4.156716823577881\n",
      "The 185 batch loss is 4.309530735015869\n",
      "The 186 batch loss is 4.12214469909668\n",
      "The 187 batch loss is 4.108486652374268\n",
      "The 188 batch loss is 4.094907283782959\n",
      "The 189 batch loss is 4.182820796966553\n",
      "The 190 batch loss is 4.3411102294921875\n",
      "The 191 batch loss is 4.425656795501709\n",
      "The 192 batch loss is 4.257846832275391\n",
      "The 193 batch loss is 3.9833288192749023\n",
      "The 194 batch loss is 3.915436267852783\n",
      "The 195 batch loss is 4.047591209411621\n",
      "The 196 batch loss is 4.288867950439453\n",
      "The 197 batch loss is 4.163059711456299\n",
      "The 198 batch loss is 4.099667072296143\n",
      "The 199 batch loss is 3.9830877780914307\n",
      "The 200 batch loss is 3.9264729022979736\n",
      "The 201 batch loss is 4.052666187286377\n",
      "The 202 batch loss is 3.9830572605133057\n",
      "The 203 batch loss is 3.9405593872070312\n",
      "The 204 batch loss is 4.0743889808654785\n",
      "The 205 batch loss is 4.381743431091309\n",
      "The 206 batch loss is 4.289920330047607\n",
      "The 207 batch loss is 4.030416011810303\n",
      "The 208 batch loss is 4.192973613739014\n",
      "The 209 batch loss is 3.928457260131836\n",
      "The 210 batch loss is 4.174119472503662\n",
      "The 211 batch loss is 4.055818557739258\n",
      "The 212 batch loss is 3.757814645767212\n",
      "The 213 batch loss is 4.239036560058594\n",
      "The 214 batch loss is 4.223435878753662\n",
      "The 215 batch loss is 4.150967121124268\n",
      "The 216 batch loss is 4.111963272094727\n",
      "The 217 batch loss is 4.027493000030518\n",
      "The 218 batch loss is 4.365244388580322\n",
      "The 219 batch loss is 4.217541217803955\n",
      "The 220 batch loss is 4.09517240524292\n",
      "The 221 batch loss is 3.973163366317749\n",
      "The 222 batch loss is 3.895444869995117\n",
      "The 223 batch loss is 4.222005844116211\n",
      "The 224 batch loss is 4.151248455047607\n",
      "The 225 batch loss is 4.059311866760254\n",
      "The 226 batch loss is 4.388018608093262\n",
      "Epoch: 03 | Time: 24m 56s\n",
      "\tTrain Loss: 4.268 | Train PPL:  71.400\n",
      "\t Val. Loss: 4.623 |  Val. PPL: 101.754\n",
      "The 0 batch loss is 4.144166946411133\n",
      "The 1 batch loss is 4.115026950836182\n",
      "The 2 batch loss is 4.2257399559021\n",
      "The 3 batch loss is 4.123844146728516\n",
      "The 4 batch loss is 4.162411689758301\n",
      "The 5 batch loss is 3.853398323059082\n",
      "The 6 batch loss is 4.1076788902282715\n",
      "The 7 batch loss is 4.030506610870361\n",
      "The 8 batch loss is 4.019533634185791\n",
      "The 9 batch loss is 4.074402809143066\n",
      "The 10 batch loss is 3.9784023761749268\n",
      "The 11 batch loss is 4.085190296173096\n",
      "The 12 batch loss is 3.814671754837036\n",
      "The 13 batch loss is 3.831812620162964\n",
      "The 14 batch loss is 4.035645008087158\n",
      "The 15 batch loss is 4.048405170440674\n",
      "The 16 batch loss is 3.805013418197632\n",
      "The 17 batch loss is 4.0662736892700195\n",
      "The 18 batch loss is 4.131829738616943\n",
      "The 19 batch loss is 3.904707670211792\n",
      "The 20 batch loss is 4.251887321472168\n",
      "The 21 batch loss is 4.348326683044434\n",
      "The 22 batch loss is 4.13681173324585\n",
      "The 23 batch loss is 3.7657742500305176\n",
      "The 24 batch loss is 4.050555229187012\n",
      "The 25 batch loss is 3.977919101715088\n",
      "The 26 batch loss is 3.930729866027832\n",
      "The 27 batch loss is 4.100322723388672\n",
      "The 28 batch loss is 3.9499495029449463\n",
      "The 29 batch loss is 4.082394599914551\n",
      "The 30 batch loss is 4.124436378479004\n",
      "The 31 batch loss is 4.51615047454834\n",
      "The 32 batch loss is 4.209117412567139\n",
      "The 33 batch loss is 3.906015634536743\n",
      "The 34 batch loss is 3.814199209213257\n",
      "The 35 batch loss is 4.040160179138184\n",
      "The 36 batch loss is 4.024594306945801\n",
      "The 37 batch loss is 4.123671531677246\n",
      "The 38 batch loss is 4.228174686431885\n",
      "The 39 batch loss is 4.068122386932373\n",
      "The 40 batch loss is 3.917036294937134\n",
      "The 41 batch loss is 4.278953552246094\n",
      "The 42 batch loss is 3.9035747051239014\n",
      "The 43 batch loss is 3.9924044609069824\n",
      "The 44 batch loss is 4.074056625366211\n",
      "The 45 batch loss is 3.9635698795318604\n",
      "The 46 batch loss is 4.054498195648193\n",
      "The 47 batch loss is 3.9610157012939453\n",
      "The 48 batch loss is 4.075863361358643\n",
      "The 49 batch loss is 3.981144428253174\n",
      "The 50 batch loss is 4.034908771514893\n",
      "The 51 batch loss is 4.067941188812256\n",
      "The 52 batch loss is 4.123718738555908\n",
      "The 53 batch loss is 3.999396800994873\n",
      "The 54 batch loss is 3.81026029586792\n",
      "The 55 batch loss is 4.265176296234131\n",
      "The 56 batch loss is 3.7531423568725586\n",
      "The 57 batch loss is 4.22716760635376\n",
      "The 58 batch loss is 4.105531215667725\n",
      "The 59 batch loss is 3.71777606010437\n",
      "The 60 batch loss is 4.02586555480957\n",
      "The 61 batch loss is 4.173123359680176\n",
      "The 62 batch loss is 4.005311965942383\n",
      "The 63 batch loss is 3.840466022491455\n",
      "The 64 batch loss is 4.140333652496338\n",
      "The 65 batch loss is 4.130836009979248\n",
      "The 66 batch loss is 4.258587837219238\n",
      "The 67 batch loss is 4.079097270965576\n",
      "The 68 batch loss is 4.214046478271484\n",
      "The 69 batch loss is 3.9495465755462646\n",
      "The 70 batch loss is 4.311410903930664\n",
      "The 71 batch loss is 4.307163238525391\n",
      "The 72 batch loss is 4.0654802322387695\n",
      "The 73 batch loss is 4.059950351715088\n",
      "The 74 batch loss is 4.016209602355957\n",
      "The 75 batch loss is 3.8831722736358643\n",
      "The 76 batch loss is 3.9307796955108643\n",
      "The 77 batch loss is 4.061727523803711\n",
      "The 78 batch loss is 4.087262153625488\n",
      "The 79 batch loss is 4.173681259155273\n",
      "The 80 batch loss is 4.206623077392578\n",
      "The 81 batch loss is 3.9271745681762695\n",
      "The 82 batch loss is 3.9329631328582764\n",
      "The 83 batch loss is 3.956225633621216\n",
      "The 84 batch loss is 3.911109209060669\n",
      "The 85 batch loss is 4.135580539703369\n",
      "The 86 batch loss is 4.053308486938477\n",
      "The 87 batch loss is 4.084200859069824\n",
      "The 88 batch loss is 3.8656418323516846\n",
      "The 89 batch loss is 3.9295859336853027\n",
      "The 90 batch loss is 3.9642069339752197\n",
      "The 91 batch loss is 4.194067001342773\n",
      "The 92 batch loss is 4.094526290893555\n",
      "The 93 batch loss is 4.310972213745117\n",
      "The 94 batch loss is 3.880486011505127\n",
      "The 95 batch loss is 3.9891085624694824\n",
      "The 96 batch loss is 3.8509857654571533\n",
      "The 97 batch loss is 3.9327392578125\n",
      "The 98 batch loss is 3.94881010055542\n",
      "The 99 batch loss is 4.167910575866699\n",
      "The 100 batch loss is 3.846862316131592\n",
      "The 101 batch loss is 3.9651975631713867\n",
      "The 102 batch loss is 4.248530387878418\n",
      "The 103 batch loss is 4.286759376525879\n",
      "The 104 batch loss is 4.048267841339111\n",
      "The 105 batch loss is 4.34010124206543\n",
      "The 106 batch loss is 3.8381705284118652\n",
      "The 107 batch loss is 4.223968029022217\n",
      "The 108 batch loss is 3.8055098056793213\n",
      "The 109 batch loss is 4.0481767654418945\n",
      "The 110 batch loss is 4.01136589050293\n",
      "The 111 batch loss is 4.045310974121094\n",
      "The 112 batch loss is 3.761796712875366\n",
      "The 113 batch loss is 4.137851238250732\n",
      "The 114 batch loss is 3.8703243732452393\n",
      "The 115 batch loss is 4.107531547546387\n",
      "The 116 batch loss is 3.9956166744232178\n",
      "The 117 batch loss is 3.915330410003662\n",
      "The 118 batch loss is 4.162863731384277\n",
      "The 119 batch loss is 4.08458137512207\n",
      "The 120 batch loss is 3.9480385780334473\n",
      "The 121 batch loss is 3.7867352962493896\n",
      "The 122 batch loss is 4.1687235832214355\n",
      "The 123 batch loss is 3.8943519592285156\n",
      "The 124 batch loss is 4.09443998336792\n",
      "The 125 batch loss is 4.022346496582031\n",
      "The 126 batch loss is 3.721581220626831\n",
      "The 127 batch loss is 4.027016639709473\n",
      "The 128 batch loss is 3.8168232440948486\n",
      "The 129 batch loss is 3.7679576873779297\n",
      "The 130 batch loss is 4.201923847198486\n",
      "The 131 batch loss is 4.102234840393066\n",
      "The 132 batch loss is 3.833716869354248\n",
      "The 133 batch loss is 3.7436985969543457\n",
      "The 134 batch loss is 3.968690872192383\n",
      "The 135 batch loss is 4.13051176071167\n",
      "The 136 batch loss is 3.9643704891204834\n",
      "The 137 batch loss is 3.844197988510132\n",
      "The 138 batch loss is 4.1461052894592285\n",
      "The 139 batch loss is 4.182225704193115\n",
      "The 140 batch loss is 3.8854963779449463\n",
      "The 141 batch loss is 3.761970281600952\n",
      "The 142 batch loss is 3.9295313358306885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 143 batch loss is 4.033021926879883\n",
      "The 144 batch loss is 4.098025321960449\n",
      "The 145 batch loss is 4.2982635498046875\n",
      "The 146 batch loss is 4.094449043273926\n",
      "The 147 batch loss is 4.156161785125732\n",
      "The 148 batch loss is 3.763213872909546\n",
      "The 149 batch loss is 3.8338475227355957\n",
      "The 150 batch loss is 3.9777514934539795\n",
      "The 151 batch loss is 3.834501028060913\n",
      "The 152 batch loss is 3.872028112411499\n",
      "The 153 batch loss is 4.017601490020752\n",
      "The 154 batch loss is 3.6380255222320557\n",
      "The 155 batch loss is 3.899747371673584\n",
      "The 156 batch loss is 3.8690738677978516\n",
      "The 157 batch loss is 3.924701452255249\n",
      "The 158 batch loss is 3.95959734916687\n",
      "The 159 batch loss is 3.7458441257476807\n",
      "The 160 batch loss is 4.267694473266602\n",
      "The 161 batch loss is 4.167255878448486\n",
      "The 162 batch loss is 3.954576253890991\n",
      "The 163 batch loss is 4.109953880310059\n",
      "The 164 batch loss is 4.117443084716797\n",
      "The 165 batch loss is 3.9337501525878906\n",
      "The 166 batch loss is 4.155518531799316\n",
      "The 167 batch loss is 3.6702616214752197\n",
      "The 168 batch loss is 4.003002643585205\n",
      "The 169 batch loss is 3.9343059062957764\n",
      "The 170 batch loss is 3.9863455295562744\n",
      "The 171 batch loss is 3.790090322494507\n",
      "The 172 batch loss is 3.6146903038024902\n",
      "The 173 batch loss is 3.7683064937591553\n",
      "The 174 batch loss is 3.673942804336548\n",
      "The 175 batch loss is 4.0840535163879395\n",
      "The 176 batch loss is 3.9351613521575928\n",
      "The 177 batch loss is 3.811187505722046\n",
      "The 178 batch loss is 3.8621649742126465\n",
      "The 179 batch loss is 3.7794883251190186\n",
      "The 180 batch loss is 3.8631157875061035\n",
      "The 181 batch loss is 4.058863639831543\n",
      "The 182 batch loss is 3.6787943840026855\n",
      "The 183 batch loss is 4.120151996612549\n",
      "The 184 batch loss is 4.062563896179199\n",
      "The 185 batch loss is 3.999019145965576\n",
      "The 186 batch loss is 4.133328914642334\n",
      "The 187 batch loss is 4.052426338195801\n",
      "The 188 batch loss is 4.221280574798584\n",
      "The 189 batch loss is 4.0335893630981445\n",
      "The 190 batch loss is 3.9495348930358887\n",
      "The 191 batch loss is 4.0087056159973145\n",
      "The 192 batch loss is 3.883791208267212\n",
      "The 193 batch loss is 3.8690145015716553\n",
      "The 194 batch loss is 4.0003509521484375\n",
      "The 195 batch loss is 3.8032238483428955\n",
      "The 196 batch loss is 3.8419313430786133\n",
      "The 197 batch loss is 3.773939847946167\n",
      "The 198 batch loss is 3.883596420288086\n",
      "The 199 batch loss is 4.1375508308410645\n",
      "The 200 batch loss is 3.7334203720092773\n",
      "The 201 batch loss is 3.749204158782959\n",
      "The 202 batch loss is 4.1756391525268555\n",
      "The 203 batch loss is 3.9630000591278076\n",
      "The 204 batch loss is 3.8030312061309814\n",
      "The 205 batch loss is 3.801140308380127\n",
      "The 206 batch loss is 4.13245964050293\n",
      "The 207 batch loss is 3.7411720752716064\n",
      "The 208 batch loss is 4.201994895935059\n",
      "The 209 batch loss is 3.9472908973693848\n",
      "The 210 batch loss is 4.075693130493164\n",
      "The 211 batch loss is 4.048031806945801\n",
      "The 212 batch loss is 4.089946746826172\n",
      "The 213 batch loss is 4.183159351348877\n",
      "The 214 batch loss is 4.181451797485352\n",
      "The 215 batch loss is 4.0111284255981445\n",
      "The 216 batch loss is 3.992633819580078\n",
      "The 217 batch loss is 3.9452648162841797\n",
      "The 218 batch loss is 3.96944260597229\n",
      "The 219 batch loss is 3.850338935852051\n",
      "The 220 batch loss is 3.864365339279175\n",
      "The 221 batch loss is 3.886019229888916\n",
      "The 222 batch loss is 3.898122787475586\n",
      "The 223 batch loss is 4.149745464324951\n",
      "The 224 batch loss is 4.167985916137695\n",
      "The 225 batch loss is 3.8307995796203613\n",
      "The 226 batch loss is 4.067475318908691\n",
      "Epoch: 04 | Time: 24m 49s\n",
      "\tTrain Loss: 4.003 | Train PPL:  54.772\n",
      "\t Val. Loss: 4.435 |  Val. PPL:  84.352\n",
      "The 0 batch loss is 4.074399471282959\n",
      "The 1 batch loss is 4.124294757843018\n",
      "The 2 batch loss is 3.999063491821289\n",
      "The 3 batch loss is 3.7606992721557617\n",
      "The 4 batch loss is 4.105731964111328\n",
      "The 5 batch loss is 3.734734296798706\n",
      "The 6 batch loss is 3.792518377304077\n",
      "The 7 batch loss is 3.8595030307769775\n",
      "The 8 batch loss is 3.681610345840454\n",
      "The 9 batch loss is 3.7759320735931396\n",
      "The 10 batch loss is 3.881241798400879\n",
      "The 11 batch loss is 3.8849968910217285\n",
      "The 12 batch loss is 3.736224889755249\n",
      "The 13 batch loss is 3.980870246887207\n",
      "The 14 batch loss is 3.9264984130859375\n",
      "The 15 batch loss is 3.8084309101104736\n",
      "The 16 batch loss is 4.17846155166626\n",
      "The 17 batch loss is 4.023519992828369\n",
      "The 18 batch loss is 3.7964515686035156\n",
      "The 19 batch loss is 3.8228111267089844\n",
      "The 20 batch loss is 3.7965075969696045\n",
      "The 21 batch loss is 3.9912703037261963\n",
      "The 22 batch loss is 4.1009626388549805\n",
      "The 23 batch loss is 3.9630117416381836\n",
      "The 24 batch loss is 3.870915412902832\n",
      "The 25 batch loss is 4.03036642074585\n",
      "The 26 batch loss is 3.8744254112243652\n",
      "The 27 batch loss is 3.856989622116089\n",
      "The 28 batch loss is 3.8322858810424805\n",
      "The 29 batch loss is 3.902374267578125\n",
      "The 30 batch loss is 3.773533821105957\n",
      "The 31 batch loss is 3.9234063625335693\n",
      "The 32 batch loss is 3.745936632156372\n",
      "The 33 batch loss is 3.5840084552764893\n",
      "The 34 batch loss is 3.997654676437378\n",
      "The 35 batch loss is 3.9504239559173584\n",
      "The 36 batch loss is 3.9672951698303223\n",
      "The 37 batch loss is 3.8071796894073486\n",
      "The 38 batch loss is 3.8887784481048584\n",
      "The 39 batch loss is 4.216825485229492\n",
      "The 40 batch loss is 3.7903804779052734\n",
      "The 41 batch loss is 3.9801948070526123\n",
      "The 42 batch loss is 3.78962779045105\n",
      "The 43 batch loss is 3.8558425903320312\n",
      "The 44 batch loss is 3.7746834754943848\n",
      "The 45 batch loss is 3.5401504039764404\n",
      "The 46 batch loss is 4.006907939910889\n",
      "The 47 batch loss is 4.274657726287842\n",
      "The 48 batch loss is 3.5408663749694824\n",
      "The 49 batch loss is 3.7883553504943848\n",
      "The 50 batch loss is 3.9780702590942383\n",
      "The 51 batch loss is 3.8610823154449463\n",
      "The 52 batch loss is 4.161459445953369\n",
      "The 53 batch loss is 3.51357102394104\n",
      "The 54 batch loss is 3.686422109603882\n",
      "The 55 batch loss is 4.016173362731934\n",
      "The 56 batch loss is 3.8249597549438477\n",
      "The 57 batch loss is 4.095457553863525\n",
      "The 58 batch loss is 3.9007182121276855\n",
      "The 59 batch loss is 3.7856926918029785\n",
      "The 60 batch loss is 3.6471030712127686\n",
      "The 61 batch loss is 4.009927749633789\n",
      "The 62 batch loss is 3.8995983600616455\n",
      "The 63 batch loss is 3.769595146179199\n",
      "The 64 batch loss is 3.6700186729431152\n",
      "The 65 batch loss is 3.767786979675293\n",
      "The 66 batch loss is 3.8037123680114746\n",
      "The 67 batch loss is 3.7892534732818604\n",
      "The 68 batch loss is 3.8149077892303467\n",
      "The 69 batch loss is 3.716150999069214\n",
      "The 70 batch loss is 3.843733549118042\n",
      "The 71 batch loss is 4.174066543579102\n",
      "The 72 batch loss is 3.7795281410217285\n",
      "The 73 batch loss is 3.9366586208343506\n",
      "The 74 batch loss is 3.8381717205047607\n",
      "The 75 batch loss is 3.9275104999542236\n",
      "The 76 batch loss is 3.9320335388183594\n",
      "The 77 batch loss is 3.6372628211975098\n",
      "The 78 batch loss is 3.7494304180145264\n",
      "The 79 batch loss is 4.2878899574279785\n",
      "The 80 batch loss is 3.9214282035827637\n",
      "The 81 batch loss is 3.6131415367126465\n",
      "The 82 batch loss is 3.664186716079712\n",
      "The 83 batch loss is 4.290764808654785\n",
      "The 84 batch loss is 3.764547109603882\n",
      "The 85 batch loss is 3.886019468307495\n",
      "The 86 batch loss is 3.891829013824463\n",
      "The 87 batch loss is 3.6396594047546387\n",
      "The 88 batch loss is 3.800920009613037\n",
      "The 89 batch loss is 3.9199411869049072\n",
      "The 90 batch loss is 3.99163556098938\n",
      "The 91 batch loss is 4.14968204498291\n",
      "The 92 batch loss is 3.7188940048217773\n",
      "The 93 batch loss is 3.6258718967437744\n",
      "The 94 batch loss is 3.909639596939087\n",
      "The 95 batch loss is 3.8372867107391357\n",
      "The 96 batch loss is 4.007349967956543\n",
      "The 97 batch loss is 3.9316484928131104\n",
      "The 98 batch loss is 3.753145217895508\n",
      "The 99 batch loss is 3.989133358001709\n",
      "The 100 batch loss is 3.922586679458618\n",
      "The 101 batch loss is 3.759033441543579\n",
      "The 102 batch loss is 3.591280698776245\n",
      "The 103 batch loss is 3.7041614055633545\n",
      "The 104 batch loss is 3.9057669639587402\n",
      "The 105 batch loss is 3.8179919719696045\n",
      "The 106 batch loss is 3.4879324436187744\n",
      "The 107 batch loss is 3.5660643577575684\n",
      "The 108 batch loss is 3.783522129058838\n",
      "The 109 batch loss is 4.138259410858154\n",
      "The 110 batch loss is 3.662195920944214\n",
      "The 111 batch loss is 3.8345727920532227\n",
      "The 112 batch loss is 3.869537591934204\n",
      "The 113 batch loss is 3.8505799770355225\n",
      "The 114 batch loss is 3.6756460666656494\n",
      "The 115 batch loss is 3.8386802673339844\n",
      "The 116 batch loss is 4.071639060974121\n",
      "The 117 batch loss is 3.854367971420288\n",
      "The 118 batch loss is 3.7754673957824707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 119 batch loss is 3.696643114089966\n",
      "The 120 batch loss is 3.658400774002075\n",
      "The 121 batch loss is 3.7597103118896484\n",
      "The 122 batch loss is 3.3438498973846436\n",
      "The 123 batch loss is 3.5431594848632812\n",
      "The 124 batch loss is 3.770787477493286\n",
      "The 125 batch loss is 3.8425347805023193\n",
      "The 126 batch loss is 3.7276768684387207\n",
      "The 127 batch loss is 3.883430242538452\n",
      "The 128 batch loss is 3.864579677581787\n",
      "The 129 batch loss is 3.71492862701416\n",
      "The 130 batch loss is 3.8233070373535156\n",
      "The 131 batch loss is 3.770671844482422\n",
      "The 132 batch loss is 3.8678715229034424\n",
      "The 133 batch loss is 3.6873040199279785\n",
      "The 134 batch loss is 3.79801607131958\n",
      "The 135 batch loss is 3.6075408458709717\n",
      "The 136 batch loss is 3.9487831592559814\n",
      "The 137 batch loss is 3.631981134414673\n",
      "The 138 batch loss is 4.290680408477783\n",
      "The 139 batch loss is 3.7343101501464844\n",
      "The 140 batch loss is 3.6792447566986084\n",
      "The 141 batch loss is 4.1345133781433105\n",
      "The 142 batch loss is 4.021808624267578\n",
      "The 143 batch loss is 3.9747321605682373\n",
      "The 144 batch loss is 3.6894888877868652\n",
      "The 145 batch loss is 3.8449535369873047\n",
      "The 146 batch loss is 3.7908473014831543\n",
      "The 147 batch loss is 3.9040169715881348\n",
      "The 148 batch loss is 3.8440678119659424\n",
      "The 149 batch loss is 3.6621930599212646\n",
      "The 150 batch loss is 4.076240062713623\n",
      "The 151 batch loss is 3.5827715396881104\n",
      "The 152 batch loss is 3.91727614402771\n",
      "The 153 batch loss is 3.696784734725952\n",
      "The 154 batch loss is 3.790128707885742\n",
      "The 155 batch loss is 4.166715621948242\n",
      "The 156 batch loss is 3.84928822517395\n",
      "The 157 batch loss is 3.7085120677948\n",
      "The 158 batch loss is 3.67901873588562\n",
      "The 159 batch loss is 3.766117811203003\n",
      "The 160 batch loss is 3.8084065914154053\n",
      "The 161 batch loss is 3.8039839267730713\n",
      "The 162 batch loss is 3.7849888801574707\n",
      "The 163 batch loss is 3.7787563800811768\n",
      "The 164 batch loss is 3.7874464988708496\n",
      "The 165 batch loss is 3.8075830936431885\n",
      "The 166 batch loss is 3.6472256183624268\n",
      "The 167 batch loss is 3.8594255447387695\n",
      "The 168 batch loss is 3.8172905445098877\n",
      "The 169 batch loss is 3.622737407684326\n",
      "The 170 batch loss is 4.013927459716797\n",
      "The 171 batch loss is 3.7374267578125\n",
      "The 172 batch loss is 3.6676902770996094\n",
      "The 173 batch loss is 3.8952836990356445\n",
      "The 174 batch loss is 4.28720235824585\n",
      "The 175 batch loss is 3.5153584480285645\n",
      "The 176 batch loss is 3.610250473022461\n",
      "The 177 batch loss is 3.7052361965179443\n",
      "The 178 batch loss is 3.6859030723571777\n",
      "The 179 batch loss is 3.682455062866211\n",
      "The 180 batch loss is 3.87609601020813\n",
      "The 181 batch loss is 3.7812130451202393\n",
      "The 182 batch loss is 3.9831206798553467\n",
      "The 183 batch loss is 3.9389963150024414\n",
      "The 184 batch loss is 3.6202456951141357\n",
      "The 185 batch loss is 3.70121169090271\n",
      "The 186 batch loss is 3.937105655670166\n",
      "The 187 batch loss is 3.750377655029297\n",
      "The 188 batch loss is 3.5650594234466553\n",
      "The 189 batch loss is 3.851979970932007\n",
      "The 190 batch loss is 3.7655017375946045\n",
      "The 191 batch loss is 3.9299232959747314\n",
      "The 192 batch loss is 3.8559603691101074\n",
      "The 193 batch loss is 3.765479803085327\n",
      "The 194 batch loss is 3.9954283237457275\n",
      "The 195 batch loss is 3.763721466064453\n",
      "The 196 batch loss is 4.010717868804932\n",
      "The 197 batch loss is 3.8609237670898438\n",
      "The 198 batch loss is 3.9467220306396484\n",
      "The 199 batch loss is 3.789085626602173\n",
      "The 200 batch loss is 3.9706687927246094\n",
      "The 201 batch loss is 3.5084972381591797\n",
      "The 202 batch loss is 3.8475277423858643\n",
      "The 203 batch loss is 3.6810519695281982\n",
      "The 204 batch loss is 3.99204158782959\n",
      "The 205 batch loss is 3.707843780517578\n",
      "The 206 batch loss is 3.6349804401397705\n",
      "The 207 batch loss is 3.4401395320892334\n",
      "The 208 batch loss is 3.733937978744507\n",
      "The 209 batch loss is 3.7934060096740723\n",
      "The 210 batch loss is 3.8259942531585693\n",
      "The 211 batch loss is 3.622204303741455\n",
      "The 212 batch loss is 3.8069710731506348\n",
      "The 213 batch loss is 3.515988349914551\n",
      "The 214 batch loss is 3.900214672088623\n",
      "The 215 batch loss is 3.7823164463043213\n",
      "The 216 batch loss is 3.6310932636260986\n",
      "The 217 batch loss is 3.8416504859924316\n",
      "The 218 batch loss is 3.663341522216797\n",
      "The 219 batch loss is 3.8247814178466797\n",
      "The 220 batch loss is 3.6992759704589844\n",
      "The 221 batch loss is 3.8458755016326904\n",
      "The 222 batch loss is 3.7085459232330322\n",
      "The 223 batch loss is 3.6277267932891846\n",
      "The 224 batch loss is 3.715555191040039\n",
      "The 225 batch loss is 3.8856635093688965\n",
      "The 226 batch loss is 3.637305736541748\n",
      "Epoch: 05 | Time: 25m 7s\n",
      "\tTrain Loss: 3.826 | Train PPL:  45.857\n",
      "\t Val. Loss: 4.402 |  Val. PPL:  81.585\n",
      "The 0 batch loss is 3.6222734451293945\n",
      "The 1 batch loss is 3.772392749786377\n",
      "The 2 batch loss is 3.5928103923797607\n",
      "The 3 batch loss is 3.366750478744507\n",
      "The 4 batch loss is 3.638411521911621\n",
      "The 5 batch loss is 3.5358190536499023\n",
      "The 6 batch loss is 4.113716125488281\n",
      "The 7 batch loss is 3.9320077896118164\n",
      "The 8 batch loss is 3.557593822479248\n",
      "The 9 batch loss is 3.645160675048828\n",
      "The 10 batch loss is 3.595170736312866\n",
      "The 11 batch loss is 3.61629319190979\n",
      "The 12 batch loss is 3.911177158355713\n",
      "The 13 batch loss is 3.5160670280456543\n",
      "The 14 batch loss is 3.670408248901367\n",
      "The 15 batch loss is 3.853867769241333\n",
      "The 16 batch loss is 3.551614284515381\n",
      "The 17 batch loss is 3.7747321128845215\n",
      "The 18 batch loss is 3.689864158630371\n",
      "The 19 batch loss is 3.8637635707855225\n",
      "The 20 batch loss is 3.5334179401397705\n",
      "The 21 batch loss is 3.79019832611084\n",
      "The 22 batch loss is 3.8731868267059326\n",
      "The 23 batch loss is 3.759422540664673\n",
      "The 24 batch loss is 3.6114554405212402\n",
      "The 25 batch loss is 3.7194607257843018\n",
      "The 26 batch loss is 3.5762696266174316\n",
      "The 27 batch loss is 3.4612057209014893\n",
      "The 28 batch loss is 3.9295430183410645\n",
      "The 29 batch loss is 3.820007562637329\n",
      "The 30 batch loss is 3.7068207263946533\n",
      "The 31 batch loss is 3.639425277709961\n",
      "The 32 batch loss is 3.70534610748291\n",
      "The 33 batch loss is 3.614997625350952\n",
      "The 34 batch loss is 3.5808517932891846\n",
      "The 35 batch loss is 3.791247606277466\n",
      "The 36 batch loss is 3.4785892963409424\n",
      "The 37 batch loss is 3.5856192111968994\n",
      "The 38 batch loss is 3.5301613807678223\n",
      "The 39 batch loss is 3.63381290435791\n",
      "The 40 batch loss is 3.68670916557312\n",
      "The 41 batch loss is 3.6820430755615234\n",
      "The 42 batch loss is 3.8412129878997803\n",
      "The 43 batch loss is 3.6526038646698\n",
      "The 44 batch loss is 3.523902654647827\n",
      "The 45 batch loss is 3.6123785972595215\n",
      "The 46 batch loss is 3.5444905757904053\n",
      "The 47 batch loss is 3.672421932220459\n",
      "The 48 batch loss is 3.9096055030822754\n",
      "The 49 batch loss is 3.645909309387207\n",
      "The 50 batch loss is 3.972959041595459\n",
      "The 51 batch loss is 3.6770362854003906\n",
      "The 52 batch loss is 3.5212621688842773\n",
      "The 53 batch loss is 3.7424938678741455\n",
      "The 54 batch loss is 3.7142977714538574\n",
      "The 55 batch loss is 3.5966105461120605\n",
      "The 56 batch loss is 3.690067768096924\n",
      "The 57 batch loss is 3.663691997528076\n",
      "The 58 batch loss is 3.7883315086364746\n",
      "The 59 batch loss is 3.5057897567749023\n",
      "The 60 batch loss is 3.9287710189819336\n",
      "The 61 batch loss is 3.876617908477783\n",
      "The 62 batch loss is 3.771540403366089\n",
      "The 63 batch loss is 3.7607741355895996\n",
      "The 64 batch loss is 4.042004585266113\n",
      "The 65 batch loss is 3.6703879833221436\n",
      "The 66 batch loss is 3.5006306171417236\n",
      "The 67 batch loss is 3.6106228828430176\n",
      "The 68 batch loss is 3.7873332500457764\n",
      "The 69 batch loss is 3.4151771068573\n",
      "The 70 batch loss is 3.8847672939300537\n",
      "The 71 batch loss is 3.729471445083618\n",
      "The 72 batch loss is 3.733351230621338\n",
      "The 73 batch loss is 3.6093311309814453\n",
      "The 74 batch loss is 3.808166265487671\n",
      "The 75 batch loss is 3.776745080947876\n",
      "The 76 batch loss is 3.6063148975372314\n",
      "The 77 batch loss is 3.773439407348633\n",
      "The 78 batch loss is 3.924124240875244\n",
      "The 79 batch loss is 3.4640634059906006\n",
      "The 80 batch loss is 3.64619517326355\n",
      "The 81 batch loss is 3.9092676639556885\n",
      "The 82 batch loss is 3.583122968673706\n",
      "The 83 batch loss is 3.759488344192505\n",
      "The 84 batch loss is 4.125011444091797\n",
      "The 85 batch loss is 3.7070090770721436\n",
      "The 86 batch loss is 3.8347692489624023\n",
      "The 87 batch loss is 3.5052998065948486\n",
      "The 88 batch loss is 3.675332546234131\n",
      "The 89 batch loss is 3.9052956104278564\n",
      "The 90 batch loss is 3.817793369293213\n",
      "The 91 batch loss is 3.8169944286346436\n",
      "The 92 batch loss is 3.9412801265716553\n",
      "The 93 batch loss is 3.5913302898406982\n",
      "The 94 batch loss is 3.577087879180908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 95 batch loss is 3.7364118099212646\n",
      "The 96 batch loss is 3.4881410598754883\n",
      "The 97 batch loss is 3.8260157108306885\n",
      "The 98 batch loss is 4.128074645996094\n",
      "The 99 batch loss is 3.881185293197632\n",
      "The 100 batch loss is 3.540809392929077\n",
      "The 101 batch loss is 3.809256076812744\n",
      "The 102 batch loss is 3.781315326690674\n",
      "The 103 batch loss is 3.778160810470581\n",
      "The 104 batch loss is 3.5176634788513184\n",
      "The 105 batch loss is 3.729886293411255\n",
      "The 106 batch loss is 3.6450741291046143\n",
      "The 107 batch loss is 3.684741497039795\n",
      "The 108 batch loss is 3.6268253326416016\n",
      "The 109 batch loss is 3.5216970443725586\n",
      "The 110 batch loss is 3.351209878921509\n",
      "The 111 batch loss is 3.7819838523864746\n",
      "The 112 batch loss is 3.777371406555176\n",
      "The 113 batch loss is 3.997359037399292\n",
      "The 114 batch loss is 3.4139771461486816\n",
      "The 115 batch loss is 3.825582265853882\n",
      "The 116 batch loss is 3.469015121459961\n",
      "The 117 batch loss is 3.7952606678009033\n",
      "The 118 batch loss is 3.8098208904266357\n",
      "The 119 batch loss is 3.7193384170532227\n",
      "The 120 batch loss is 3.9870548248291016\n",
      "The 121 batch loss is 3.650918960571289\n",
      "The 122 batch loss is 3.7018656730651855\n",
      "The 123 batch loss is 3.4873673915863037\n",
      "The 124 batch loss is 3.4868574142456055\n",
      "The 125 batch loss is 3.8962907791137695\n",
      "The 126 batch loss is 3.616743564605713\n",
      "The 127 batch loss is 3.5392613410949707\n",
      "The 128 batch loss is 3.5761587619781494\n",
      "The 129 batch loss is 4.097118377685547\n",
      "The 130 batch loss is 3.5257012844085693\n",
      "The 131 batch loss is 3.702310800552368\n",
      "The 132 batch loss is 3.392186403274536\n",
      "The 133 batch loss is 3.5931007862091064\n",
      "The 134 batch loss is 4.093141555786133\n",
      "The 135 batch loss is 3.753849506378174\n",
      "The 136 batch loss is 3.5247912406921387\n",
      "The 137 batch loss is 3.5827484130859375\n",
      "The 138 batch loss is 3.8723764419555664\n",
      "The 139 batch loss is 3.515810012817383\n",
      "The 140 batch loss is 3.8887174129486084\n",
      "The 141 batch loss is 3.4720492362976074\n",
      "The 142 batch loss is 3.523181438446045\n",
      "The 143 batch loss is 3.4922735691070557\n",
      "The 144 batch loss is 3.471569299697876\n",
      "The 145 batch loss is 3.626049280166626\n",
      "The 146 batch loss is 3.7417521476745605\n",
      "The 147 batch loss is 3.9500529766082764\n",
      "The 148 batch loss is 3.5894691944122314\n",
      "The 149 batch loss is 3.6300711631774902\n",
      "The 150 batch loss is 3.5087358951568604\n",
      "The 151 batch loss is 3.7714765071868896\n",
      "The 152 batch loss is 3.7243220806121826\n",
      "The 153 batch loss is 3.7069685459136963\n",
      "The 154 batch loss is 3.545637845993042\n",
      "The 155 batch loss is 3.734065532684326\n",
      "The 156 batch loss is 3.8833653926849365\n",
      "The 157 batch loss is 3.3833863735198975\n",
      "The 158 batch loss is 4.081654071807861\n",
      "The 159 batch loss is 3.738131046295166\n",
      "The 160 batch loss is 3.720014810562134\n",
      "The 161 batch loss is 3.3896892070770264\n",
      "The 162 batch loss is 3.582664728164673\n",
      "The 163 batch loss is 3.8964357376098633\n",
      "The 164 batch loss is 3.292680025100708\n",
      "The 165 batch loss is 3.589414596557617\n",
      "The 166 batch loss is 3.48030948638916\n",
      "The 167 batch loss is 3.5589852333068848\n",
      "The 168 batch loss is 3.904937505722046\n",
      "The 169 batch loss is 3.5166494846343994\n",
      "The 170 batch loss is 4.070432186126709\n",
      "The 171 batch loss is 3.6209371089935303\n",
      "The 172 batch loss is 3.58683180809021\n",
      "The 173 batch loss is 3.9078452587127686\n",
      "The 174 batch loss is 3.72212290763855\n",
      "The 175 batch loss is 3.833649158477783\n",
      "The 176 batch loss is 3.54835844039917\n",
      "The 177 batch loss is 3.8866281509399414\n",
      "The 178 batch loss is 3.8655354976654053\n",
      "The 179 batch loss is 3.6589267253875732\n",
      "The 180 batch loss is 3.4512743949890137\n",
      "The 181 batch loss is 3.745837926864624\n",
      "The 182 batch loss is 3.3704445362091064\n",
      "The 183 batch loss is 3.7917580604553223\n",
      "The 184 batch loss is 3.6123807430267334\n",
      "The 185 batch loss is 3.4651856422424316\n",
      "The 186 batch loss is 3.509152889251709\n",
      "The 187 batch loss is 3.5505611896514893\n",
      "The 188 batch loss is 3.6443076133728027\n",
      "The 189 batch loss is 3.8618052005767822\n",
      "The 190 batch loss is 3.5258829593658447\n",
      "The 191 batch loss is 3.5325052738189697\n",
      "The 192 batch loss is 3.7556512355804443\n",
      "The 193 batch loss is 3.915717124938965\n",
      "The 194 batch loss is 3.5840232372283936\n",
      "The 195 batch loss is 3.771303176879883\n",
      "The 196 batch loss is 3.6809585094451904\n",
      "The 197 batch loss is 3.777733325958252\n",
      "The 198 batch loss is 3.800137758255005\n",
      "The 199 batch loss is 3.3903675079345703\n",
      "The 200 batch loss is 3.731769561767578\n",
      "The 201 batch loss is 3.755188465118408\n",
      "The 202 batch loss is 3.7880873680114746\n",
      "The 203 batch loss is 3.5920984745025635\n",
      "The 204 batch loss is 3.9951324462890625\n",
      "The 205 batch loss is 3.6234915256500244\n",
      "The 206 batch loss is 4.068845272064209\n",
      "The 207 batch loss is 3.643040180206299\n",
      "The 208 batch loss is 3.511631488800049\n",
      "The 209 batch loss is 3.6984825134277344\n",
      "The 210 batch loss is 3.5481138229370117\n",
      "The 211 batch loss is 3.683382511138916\n",
      "The 212 batch loss is 3.5549824237823486\n",
      "The 213 batch loss is 3.576655864715576\n",
      "The 214 batch loss is 3.596750497817993\n",
      "The 215 batch loss is 3.6944081783294678\n",
      "The 216 batch loss is 3.4034104347229004\n",
      "The 217 batch loss is 3.7720344066619873\n",
      "The 218 batch loss is 3.537281036376953\n",
      "The 219 batch loss is 3.4128692150115967\n",
      "The 220 batch loss is 3.6867010593414307\n",
      "The 221 batch loss is 3.6277213096618652\n",
      "The 222 batch loss is 3.7293100357055664\n",
      "The 223 batch loss is 3.9714105129241943\n",
      "The 224 batch loss is 3.6183524131774902\n",
      "The 225 batch loss is 3.551109552383423\n",
      "The 226 batch loss is 3.6729536056518555\n",
      "Epoch: 06 | Time: 25m 9s\n",
      "\tTrain Loss: 3.687 | Train PPL:  39.931\n",
      "\t Val. Loss: 4.277 |  Val. PPL:  72.026\n",
      "The 0 batch loss is 3.558290481567383\n",
      "The 1 batch loss is 3.367889881134033\n",
      "The 2 batch loss is 3.3426737785339355\n",
      "The 3 batch loss is 3.366387128829956\n",
      "The 4 batch loss is 3.7787749767303467\n",
      "The 5 batch loss is 3.418776273727417\n",
      "The 6 batch loss is 3.477971076965332\n",
      "The 7 batch loss is 3.7019379138946533\n",
      "The 8 batch loss is 3.464674711227417\n",
      "The 9 batch loss is 3.547123908996582\n",
      "The 10 batch loss is 3.4395670890808105\n",
      "The 11 batch loss is 3.39670991897583\n",
      "The 12 batch loss is 3.370473861694336\n",
      "The 13 batch loss is 3.6167027950286865\n",
      "The 14 batch loss is 3.756196975708008\n",
      "The 15 batch loss is 3.6190547943115234\n",
      "The 16 batch loss is 3.334933042526245\n",
      "The 17 batch loss is 3.6456856727600098\n",
      "The 18 batch loss is 3.455986976623535\n",
      "The 19 batch loss is 3.8401429653167725\n",
      "The 20 batch loss is 3.7222657203674316\n",
      "The 21 batch loss is 3.457871913909912\n",
      "The 22 batch loss is 3.7260942459106445\n",
      "The 23 batch loss is 3.7822787761688232\n",
      "The 24 batch loss is 3.7190871238708496\n",
      "The 25 batch loss is 3.622149705886841\n",
      "The 26 batch loss is 3.3875062465667725\n",
      "The 27 batch loss is 3.7288291454315186\n",
      "The 28 batch loss is 3.634214162826538\n",
      "The 29 batch loss is 3.3123769760131836\n",
      "The 30 batch loss is 3.5396058559417725\n",
      "The 31 batch loss is 3.6945934295654297\n",
      "The 32 batch loss is 3.8376786708831787\n",
      "The 33 batch loss is 3.366506338119507\n",
      "The 34 batch loss is 3.716043472290039\n",
      "The 35 batch loss is 3.5419440269470215\n",
      "The 36 batch loss is 3.6154396533966064\n",
      "The 37 batch loss is 3.667604923248291\n",
      "The 38 batch loss is 3.4647750854492188\n",
      "The 39 batch loss is 3.7021677494049072\n",
      "The 40 batch loss is 3.602713108062744\n",
      "The 41 batch loss is 3.4714043140411377\n",
      "The 42 batch loss is 3.3449294567108154\n",
      "The 43 batch loss is 3.4806132316589355\n",
      "The 44 batch loss is 3.5364785194396973\n",
      "The 45 batch loss is 3.634392738342285\n",
      "The 46 batch loss is 3.2370078563690186\n",
      "The 47 batch loss is 3.396512985229492\n",
      "The 48 batch loss is 3.6348674297332764\n",
      "The 49 batch loss is 3.6358394622802734\n",
      "The 50 batch loss is 3.4371252059936523\n",
      "The 51 batch loss is 3.438145399093628\n",
      "The 52 batch loss is 3.3897480964660645\n",
      "The 53 batch loss is 3.3030624389648438\n",
      "The 54 batch loss is 3.9570915699005127\n",
      "The 55 batch loss is 3.500636577606201\n",
      "The 56 batch loss is 3.3255615234375\n",
      "The 57 batch loss is 3.524108409881592\n",
      "The 58 batch loss is 3.7951176166534424\n",
      "The 59 batch loss is 3.53509521484375\n",
      "The 60 batch loss is 3.4374585151672363\n",
      "The 61 batch loss is 3.441815137863159\n",
      "The 62 batch loss is 3.16463565826416\n",
      "The 63 batch loss is 3.4933362007141113\n",
      "The 64 batch loss is 3.734276533126831\n",
      "The 65 batch loss is 3.4127955436706543\n",
      "The 66 batch loss is 3.6495182514190674\n",
      "The 67 batch loss is 3.609299659729004\n",
      "The 68 batch loss is 3.8024158477783203\n",
      "The 69 batch loss is 3.6908087730407715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 70 batch loss is 3.3754324913024902\n",
      "The 71 batch loss is 3.5493974685668945\n",
      "The 72 batch loss is 3.517956018447876\n",
      "The 73 batch loss is 3.509089469909668\n",
      "The 74 batch loss is 3.2330641746520996\n",
      "The 75 batch loss is 3.772731304168701\n",
      "The 76 batch loss is 3.641932487487793\n",
      "The 77 batch loss is 3.517904281616211\n",
      "The 78 batch loss is 3.7057511806488037\n",
      "The 79 batch loss is 3.5159432888031006\n",
      "The 80 batch loss is 3.36368989944458\n",
      "The 81 batch loss is 3.4531030654907227\n",
      "The 82 batch loss is 3.3277313709259033\n",
      "The 83 batch loss is 3.3838977813720703\n",
      "The 84 batch loss is 3.7549831867218018\n",
      "The 85 batch loss is 3.5066277980804443\n",
      "The 86 batch loss is 3.8696632385253906\n",
      "The 87 batch loss is 3.313004732131958\n",
      "The 88 batch loss is 3.2742819786071777\n",
      "The 89 batch loss is 3.412674903869629\n",
      "The 90 batch loss is 3.772923707962036\n",
      "The 91 batch loss is 3.324510335922241\n",
      "The 92 batch loss is 3.477475643157959\n",
      "The 93 batch loss is 3.4082701206207275\n",
      "The 94 batch loss is 3.5108273029327393\n",
      "The 95 batch loss is 3.584820508956909\n",
      "The 96 batch loss is 3.62174654006958\n",
      "The 97 batch loss is 3.60070538520813\n",
      "The 98 batch loss is 3.5653254985809326\n",
      "The 99 batch loss is 3.657228708267212\n",
      "The 100 batch loss is 3.2436771392822266\n",
      "The 101 batch loss is 3.4435248374938965\n",
      "The 102 batch loss is 3.923007011413574\n",
      "The 103 batch loss is 3.567385673522949\n",
      "The 104 batch loss is 3.2860209941864014\n",
      "The 105 batch loss is 3.489719867706299\n",
      "The 106 batch loss is 4.001290798187256\n",
      "The 107 batch loss is 3.6669936180114746\n",
      "The 108 batch loss is 3.6712141036987305\n",
      "The 109 batch loss is 3.5388495922088623\n",
      "The 110 batch loss is 3.2853872776031494\n",
      "The 111 batch loss is 3.953167200088501\n",
      "The 112 batch loss is 3.7117583751678467\n",
      "The 113 batch loss is 3.7024807929992676\n",
      "The 114 batch loss is 3.5138862133026123\n",
      "The 115 batch loss is 3.4532532691955566\n",
      "The 116 batch loss is 3.7276830673217773\n",
      "The 117 batch loss is 3.5462985038757324\n",
      "The 118 batch loss is 3.582665205001831\n",
      "The 119 batch loss is 3.3774027824401855\n",
      "The 120 batch loss is 3.5259339809417725\n",
      "The 121 batch loss is 3.1860833168029785\n",
      "The 122 batch loss is 3.4415485858917236\n",
      "The 123 batch loss is 3.720275402069092\n",
      "The 124 batch loss is 3.699571371078491\n",
      "The 125 batch loss is 3.967149019241333\n",
      "The 126 batch loss is 3.317135810852051\n",
      "The 127 batch loss is 3.3577919006347656\n",
      "The 128 batch loss is 3.357186794281006\n",
      "The 129 batch loss is 3.6962363719940186\n",
      "The 130 batch loss is 3.588512897491455\n",
      "The 131 batch loss is 3.7298927307128906\n",
      "The 132 batch loss is 3.5473783016204834\n",
      "The 133 batch loss is 3.40864634513855\n",
      "The 134 batch loss is 3.6691784858703613\n",
      "The 135 batch loss is 3.6929636001586914\n",
      "The 136 batch loss is 3.86112117767334\n",
      "The 137 batch loss is 3.684128761291504\n",
      "The 138 batch loss is 3.3987064361572266\n",
      "The 139 batch loss is 3.5602142810821533\n",
      "The 140 batch loss is 3.6572678089141846\n",
      "The 141 batch loss is 3.4279487133026123\n",
      "The 142 batch loss is 3.3970413208007812\n",
      "The 143 batch loss is 3.8752877712249756\n",
      "The 144 batch loss is 3.3936119079589844\n",
      "The 145 batch loss is 3.4851629734039307\n",
      "The 146 batch loss is 3.508571147918701\n",
      "The 147 batch loss is 3.6970248222351074\n",
      "The 148 batch loss is 3.6220927238464355\n",
      "The 149 batch loss is 3.5798308849334717\n",
      "The 150 batch loss is 3.567237615585327\n",
      "The 151 batch loss is 3.2673628330230713\n",
      "The 152 batch loss is 3.489215850830078\n",
      "The 153 batch loss is 3.630828380584717\n",
      "The 154 batch loss is 3.8144869804382324\n",
      "The 155 batch loss is 3.3936681747436523\n",
      "The 156 batch loss is 3.523578405380249\n",
      "The 157 batch loss is 3.549241781234741\n",
      "The 158 batch loss is 3.5602240562438965\n",
      "The 159 batch loss is 3.4462809562683105\n",
      "The 160 batch loss is 3.687365770339966\n",
      "The 161 batch loss is 3.5110771656036377\n",
      "The 162 batch loss is 3.6697568893432617\n",
      "The 163 batch loss is 3.240326166152954\n",
      "The 164 batch loss is 3.5897934436798096\n",
      "The 165 batch loss is 3.5007829666137695\n",
      "The 166 batch loss is 3.4283061027526855\n",
      "The 167 batch loss is 4.009714603424072\n",
      "The 168 batch loss is 3.4214324951171875\n",
      "The 169 batch loss is 3.4586875438690186\n",
      "The 170 batch loss is 3.38403582572937\n",
      "The 171 batch loss is 3.462618827819824\n",
      "The 172 batch loss is 3.48079776763916\n",
      "The 173 batch loss is 3.7047812938690186\n",
      "The 174 batch loss is 3.759551525115967\n",
      "The 175 batch loss is 3.2794790267944336\n",
      "The 176 batch loss is 3.5806028842926025\n",
      "The 177 batch loss is 3.6503896713256836\n",
      "The 178 batch loss is 3.353858470916748\n",
      "The 179 batch loss is 3.637861728668213\n",
      "The 180 batch loss is 3.4083476066589355\n",
      "The 181 batch loss is 3.3738274574279785\n",
      "The 182 batch loss is 3.595974922180176\n",
      "The 183 batch loss is 3.259887456893921\n",
      "The 184 batch loss is 3.3477845191955566\n",
      "The 185 batch loss is 3.427293062210083\n",
      "The 186 batch loss is 3.4633941650390625\n",
      "The 187 batch loss is 3.5253422260284424\n",
      "The 188 batch loss is 3.5908446311950684\n",
      "The 189 batch loss is 3.8362154960632324\n",
      "The 190 batch loss is 3.865346908569336\n",
      "The 191 batch loss is 3.678288221359253\n",
      "The 192 batch loss is 3.592780351638794\n",
      "The 193 batch loss is 3.2752294540405273\n",
      "The 194 batch loss is 3.814760684967041\n",
      "The 195 batch loss is 3.520599842071533\n",
      "The 196 batch loss is 3.6353466510772705\n",
      "The 197 batch loss is 3.435140609741211\n",
      "The 198 batch loss is 3.342034339904785\n",
      "The 199 batch loss is 3.5322165489196777\n",
      "The 200 batch loss is 3.5216574668884277\n",
      "The 201 batch loss is 3.5189027786254883\n",
      "The 202 batch loss is 3.8441529273986816\n",
      "The 203 batch loss is 3.7268643379211426\n",
      "The 204 batch loss is 3.4850149154663086\n",
      "The 205 batch loss is 3.233983278274536\n",
      "The 206 batch loss is 3.3842995166778564\n",
      "The 207 batch loss is 3.1355743408203125\n",
      "The 208 batch loss is 3.7163686752319336\n",
      "The 209 batch loss is 3.6084084510803223\n",
      "The 210 batch loss is 3.398969888687134\n",
      "The 211 batch loss is 3.3764946460723877\n",
      "The 212 batch loss is 3.577666997909546\n",
      "The 213 batch loss is 3.378352642059326\n",
      "The 214 batch loss is 3.808952808380127\n",
      "The 215 batch loss is 3.376709222793579\n",
      "The 216 batch loss is 3.2676188945770264\n",
      "The 217 batch loss is 3.242699384689331\n",
      "The 218 batch loss is 3.471733808517456\n",
      "The 219 batch loss is 3.2040886878967285\n",
      "The 220 batch loss is 3.6811728477478027\n",
      "The 221 batch loss is 3.403838634490967\n",
      "The 222 batch loss is 3.558973550796509\n",
      "The 223 batch loss is 3.354978561401367\n",
      "The 224 batch loss is 3.6434149742126465\n",
      "The 225 batch loss is 3.710463285446167\n",
      "The 226 batch loss is 3.5575132369995117\n",
      "Epoch: 07 | Time: 24m 53s\n",
      "\tTrain Loss: 3.539 | Train PPL:  34.437\n",
      "\t Val. Loss: 4.170 |  Val. PPL:  64.713\n",
      "The 0 batch loss is 3.3383591175079346\n",
      "The 1 batch loss is 3.36857008934021\n",
      "The 2 batch loss is 3.4181950092315674\n",
      "The 3 batch loss is 3.4367103576660156\n",
      "The 4 batch loss is 3.666091203689575\n",
      "The 5 batch loss is 3.3829288482666016\n",
      "The 6 batch loss is 3.4490222930908203\n",
      "The 7 batch loss is 3.4688661098480225\n",
      "The 8 batch loss is 3.40881085395813\n",
      "The 9 batch loss is 3.2408666610717773\n",
      "The 10 batch loss is 3.3909757137298584\n",
      "The 11 batch loss is 3.576368808746338\n",
      "The 12 batch loss is 3.599600076675415\n",
      "The 13 batch loss is 3.238783359527588\n",
      "The 14 batch loss is 3.445969343185425\n",
      "The 15 batch loss is 3.4231040477752686\n",
      "The 16 batch loss is 3.38053035736084\n",
      "The 17 batch loss is 3.4811997413635254\n",
      "The 18 batch loss is 3.682480812072754\n",
      "The 19 batch loss is 3.4272706508636475\n",
      "The 20 batch loss is 3.373427152633667\n",
      "The 21 batch loss is 3.4512298107147217\n",
      "The 22 batch loss is 3.3564672470092773\n",
      "The 23 batch loss is 3.486158847808838\n",
      "The 24 batch loss is 3.4846386909484863\n",
      "The 25 batch loss is 3.4034039974212646\n",
      "The 26 batch loss is 3.468186140060425\n",
      "The 27 batch loss is 3.236952781677246\n",
      "The 28 batch loss is 3.6168582439422607\n",
      "The 29 batch loss is 3.6471548080444336\n",
      "The 30 batch loss is 3.238642692565918\n",
      "The 31 batch loss is 3.287994861602783\n",
      "The 32 batch loss is 3.4760780334472656\n",
      "The 33 batch loss is 3.3501248359680176\n",
      "The 34 batch loss is 3.765059471130371\n",
      "The 35 batch loss is 3.0431535243988037\n",
      "The 36 batch loss is 3.434640645980835\n",
      "The 37 batch loss is 3.4163405895233154\n",
      "The 38 batch loss is 3.426884174346924\n",
      "The 39 batch loss is 3.1553964614868164\n",
      "The 40 batch loss is 3.1836342811584473\n",
      "The 41 batch loss is 3.6195311546325684\n",
      "The 42 batch loss is 3.319704294204712\n",
      "The 43 batch loss is 3.585052490234375\n",
      "The 44 batch loss is 3.537560224533081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 45 batch loss is 3.231532096862793\n",
      "The 46 batch loss is 3.410384178161621\n",
      "The 47 batch loss is 3.4966461658477783\n",
      "The 48 batch loss is 3.2455992698669434\n",
      "The 49 batch loss is 3.4959897994995117\n",
      "The 50 batch loss is 3.258225679397583\n",
      "The 51 batch loss is 3.3361918926239014\n",
      "The 52 batch loss is 2.9442825317382812\n",
      "The 53 batch loss is 3.950249433517456\n",
      "The 54 batch loss is 3.7281887531280518\n",
      "The 55 batch loss is 3.304412364959717\n",
      "The 56 batch loss is 3.280369281768799\n",
      "The 57 batch loss is 3.687957763671875\n",
      "The 58 batch loss is 3.759880542755127\n",
      "The 59 batch loss is 3.435725212097168\n",
      "The 60 batch loss is 3.578972816467285\n",
      "The 61 batch loss is 3.532464027404785\n",
      "The 62 batch loss is 3.2556543350219727\n",
      "The 63 batch loss is 3.5143375396728516\n",
      "The 64 batch loss is 3.6539433002471924\n",
      "The 65 batch loss is 3.356271982192993\n",
      "The 66 batch loss is 3.5853617191314697\n",
      "The 67 batch loss is 3.5372796058654785\n",
      "The 68 batch loss is 3.684807300567627\n",
      "The 69 batch loss is 3.3698835372924805\n",
      "The 70 batch loss is 3.6342644691467285\n",
      "The 71 batch loss is 3.3847315311431885\n",
      "The 72 batch loss is 3.435595750808716\n",
      "The 73 batch loss is 3.670659303665161\n",
      "The 74 batch loss is 3.602980136871338\n",
      "The 75 batch loss is 3.1317689418792725\n",
      "The 76 batch loss is 3.7451252937316895\n",
      "The 77 batch loss is 3.509080410003662\n",
      "The 78 batch loss is 3.4489710330963135\n",
      "The 79 batch loss is 3.256560802459717\n",
      "The 80 batch loss is 3.5147547721862793\n",
      "The 81 batch loss is 3.4056782722473145\n",
      "The 82 batch loss is 3.405630350112915\n",
      "The 83 batch loss is 3.4376699924468994\n",
      "The 84 batch loss is 3.2726404666900635\n",
      "The 85 batch loss is 3.4036853313446045\n",
      "The 86 batch loss is 3.500333309173584\n",
      "The 87 batch loss is 3.6290154457092285\n",
      "The 88 batch loss is 3.5785670280456543\n",
      "The 89 batch loss is 3.616783857345581\n",
      "The 90 batch loss is 3.641308307647705\n",
      "The 91 batch loss is 3.3264658451080322\n",
      "The 92 batch loss is 3.5024008750915527\n",
      "The 93 batch loss is 3.5872743129730225\n",
      "The 94 batch loss is 3.6595540046691895\n",
      "The 95 batch loss is 3.1701083183288574\n",
      "The 96 batch loss is 3.4900319576263428\n",
      "The 97 batch loss is 3.447375774383545\n",
      "The 98 batch loss is 3.3369481563568115\n",
      "The 99 batch loss is 3.1865949630737305\n",
      "The 100 batch loss is 3.1244113445281982\n",
      "The 101 batch loss is 3.3445427417755127\n",
      "The 102 batch loss is 3.21899676322937\n",
      "The 103 batch loss is 3.437481164932251\n",
      "The 104 batch loss is 3.5252768993377686\n",
      "The 105 batch loss is 3.641840696334839\n",
      "The 106 batch loss is 3.7332799434661865\n",
      "The 107 batch loss is 3.320547580718994\n",
      "The 108 batch loss is 3.097383737564087\n",
      "The 109 batch loss is 3.2106666564941406\n",
      "The 110 batch loss is 3.3865416049957275\n",
      "The 111 batch loss is 3.299708366394043\n",
      "The 112 batch loss is 3.121424913406372\n",
      "The 113 batch loss is 3.3261566162109375\n",
      "The 114 batch loss is 3.214259147644043\n",
      "The 115 batch loss is 3.3211255073547363\n",
      "The 116 batch loss is 3.529017686843872\n",
      "The 117 batch loss is 3.542302370071411\n",
      "The 118 batch loss is 3.3211936950683594\n",
      "The 119 batch loss is 3.2161073684692383\n",
      "The 120 batch loss is 3.186992883682251\n",
      "The 121 batch loss is 3.589576005935669\n",
      "The 122 batch loss is 3.3344428539276123\n",
      "The 123 batch loss is 3.693826913833618\n",
      "The 124 batch loss is 3.1075267791748047\n",
      "The 125 batch loss is 3.2366583347320557\n",
      "The 126 batch loss is 3.443617343902588\n",
      "The 127 batch loss is 3.4318923950195312\n",
      "The 128 batch loss is 3.4435765743255615\n",
      "The 129 batch loss is 3.1264123916625977\n",
      "The 130 batch loss is 3.1733407974243164\n",
      "The 131 batch loss is 3.2955660820007324\n",
      "The 132 batch loss is 3.109469175338745\n",
      "The 133 batch loss is 3.148094654083252\n",
      "The 134 batch loss is 3.2006285190582275\n",
      "The 135 batch loss is 3.230670690536499\n",
      "The 136 batch loss is 3.666411876678467\n",
      "The 137 batch loss is 3.435387134552002\n",
      "The 138 batch loss is 3.4070937633514404\n",
      "The 139 batch loss is 3.370260000228882\n",
      "The 140 batch loss is 3.247427225112915\n",
      "The 141 batch loss is 3.7325496673583984\n",
      "The 142 batch loss is 3.261338233947754\n",
      "The 143 batch loss is 3.4088521003723145\n",
      "The 144 batch loss is 3.1615777015686035\n",
      "The 145 batch loss is 3.2732911109924316\n",
      "The 146 batch loss is 3.0929007530212402\n",
      "The 147 batch loss is 3.3276121616363525\n",
      "The 148 batch loss is 3.505189895629883\n",
      "The 149 batch loss is 3.2685937881469727\n",
      "The 150 batch loss is 3.1025726795196533\n",
      "The 151 batch loss is 3.3639891147613525\n",
      "The 152 batch loss is 3.379333734512329\n",
      "The 153 batch loss is 3.4316117763519287\n",
      "The 154 batch loss is 3.5261728763580322\n",
      "The 155 batch loss is 3.699211597442627\n",
      "The 156 batch loss is 3.159958839416504\n",
      "The 157 batch loss is 3.6770637035369873\n",
      "The 158 batch loss is 3.2987923622131348\n",
      "The 159 batch loss is 3.259221315383911\n",
      "The 160 batch loss is 3.078744888305664\n",
      "The 161 batch loss is 3.4501519203186035\n",
      "The 162 batch loss is 3.1935606002807617\n",
      "The 163 batch loss is 3.0634820461273193\n",
      "The 164 batch loss is 3.1587929725646973\n",
      "The 165 batch loss is 3.2844016551971436\n",
      "The 166 batch loss is 3.3790783882141113\n",
      "The 167 batch loss is 3.500375986099243\n",
      "The 168 batch loss is 3.170013666152954\n",
      "The 169 batch loss is 3.084301471710205\n",
      "The 170 batch loss is 3.251068353652954\n",
      "The 171 batch loss is 3.3526611328125\n",
      "The 172 batch loss is 3.043853282928467\n",
      "The 173 batch loss is 3.4073476791381836\n",
      "The 174 batch loss is 3.425339937210083\n",
      "The 175 batch loss is 3.6944315433502197\n",
      "The 176 batch loss is 3.539929151535034\n",
      "The 177 batch loss is 3.5473999977111816\n",
      "The 178 batch loss is 3.7763254642486572\n",
      "The 179 batch loss is 3.1382627487182617\n",
      "The 180 batch loss is 3.2622461318969727\n",
      "The 181 batch loss is 3.2844269275665283\n",
      "The 182 batch loss is 3.503937244415283\n",
      "The 183 batch loss is 3.3963100910186768\n",
      "The 184 batch loss is 3.357076406478882\n",
      "The 185 batch loss is 3.1898794174194336\n",
      "The 186 batch loss is 3.337862730026245\n",
      "The 187 batch loss is 3.2619173526763916\n",
      "The 188 batch loss is 3.4819514751434326\n",
      "The 189 batch loss is 3.402639865875244\n",
      "The 190 batch loss is 3.564805030822754\n",
      "The 191 batch loss is 3.1804559230804443\n",
      "The 192 batch loss is 3.410552740097046\n",
      "The 193 batch loss is 3.3335931301116943\n",
      "The 194 batch loss is 3.673281669616699\n",
      "The 195 batch loss is 3.3937811851501465\n",
      "The 196 batch loss is 3.4286932945251465\n",
      "The 197 batch loss is 3.19893217086792\n",
      "The 198 batch loss is 3.3514227867126465\n",
      "The 199 batch loss is 3.2836132049560547\n",
      "The 200 batch loss is 3.2231476306915283\n",
      "The 201 batch loss is 3.253403425216675\n",
      "The 202 batch loss is 3.427031993865967\n",
      "The 203 batch loss is 3.6412556171417236\n",
      "The 204 batch loss is 3.4590415954589844\n",
      "The 205 batch loss is 3.502441167831421\n",
      "The 206 batch loss is 3.704951524734497\n",
      "The 207 batch loss is 3.2714691162109375\n",
      "The 208 batch loss is 3.456974506378174\n",
      "The 209 batch loss is 3.218471050262451\n",
      "The 210 batch loss is 3.6941919326782227\n",
      "The 211 batch loss is 3.207674026489258\n",
      "The 212 batch loss is 3.323906183242798\n",
      "The 213 batch loss is 3.4282312393188477\n",
      "The 214 batch loss is 3.4661998748779297\n",
      "The 215 batch loss is 3.4652113914489746\n",
      "The 216 batch loss is 3.2383878231048584\n",
      "The 217 batch loss is 3.283095359802246\n",
      "The 218 batch loss is 3.275775194168091\n",
      "The 219 batch loss is 3.552877426147461\n",
      "The 220 batch loss is 3.3415307998657227\n",
      "The 221 batch loss is 3.175835609436035\n",
      "The 222 batch loss is 3.455169439315796\n",
      "The 223 batch loss is 3.52828049659729\n",
      "The 224 batch loss is 3.2412729263305664\n",
      "The 225 batch loss is 3.126579999923706\n",
      "The 226 batch loss is 3.2074074745178223\n",
      "Epoch: 08 | Time: 25m 7s\n",
      "\tTrain Loss: 3.394 | Train PPL:  29.793\n",
      "\t Val. Loss: 4.019 |  Val. PPL:  55.624\n",
      "The 0 batch loss is 3.243868112564087\n",
      "The 1 batch loss is 2.9501283168792725\n",
      "The 2 batch loss is 3.675602436065674\n",
      "The 3 batch loss is 3.027939558029175\n",
      "The 4 batch loss is 3.3415894508361816\n",
      "The 5 batch loss is 3.36781907081604\n",
      "The 6 batch loss is 3.138016939163208\n",
      "The 7 batch loss is 3.5965161323547363\n",
      "The 8 batch loss is 3.02599835395813\n",
      "The 9 batch loss is 3.0076024532318115\n",
      "The 10 batch loss is 3.3175649642944336\n",
      "The 11 batch loss is 3.3241307735443115\n",
      "The 12 batch loss is 3.3359103202819824\n",
      "The 13 batch loss is 3.383164644241333\n",
      "The 14 batch loss is 3.5177805423736572\n",
      "The 15 batch loss is 3.1287100315093994\n",
      "The 16 batch loss is 3.564776659011841\n",
      "The 17 batch loss is 3.0618786811828613\n",
      "The 18 batch loss is 3.5142147541046143\n",
      "The 19 batch loss is 3.526898145675659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 20 batch loss is 3.8712539672851562\n",
      "The 21 batch loss is 3.5050997734069824\n",
      "The 22 batch loss is 3.434964179992676\n",
      "The 23 batch loss is 3.4981441497802734\n",
      "The 24 batch loss is 3.1887590885162354\n",
      "The 25 batch loss is 3.472649574279785\n",
      "The 26 batch loss is 3.3422698974609375\n",
      "The 27 batch loss is 3.27793288230896\n",
      "The 28 batch loss is 3.0221009254455566\n",
      "The 29 batch loss is 3.3563284873962402\n",
      "The 30 batch loss is 3.22770357131958\n",
      "The 31 batch loss is 2.981278896331787\n",
      "The 32 batch loss is 3.2826895713806152\n",
      "The 33 batch loss is 3.0839321613311768\n",
      "The 34 batch loss is 3.510685920715332\n",
      "The 35 batch loss is 3.0990681648254395\n",
      "The 36 batch loss is 3.0563879013061523\n",
      "The 37 batch loss is 3.0412447452545166\n",
      "The 38 batch loss is 3.1715035438537598\n",
      "The 39 batch loss is 3.224090337753296\n",
      "The 40 batch loss is 3.467590808868408\n",
      "The 41 batch loss is 3.3159754276275635\n",
      "The 42 batch loss is 3.096254825592041\n",
      "The 43 batch loss is 3.1249289512634277\n",
      "The 44 batch loss is 3.4180502891540527\n",
      "The 45 batch loss is 3.4708356857299805\n",
      "The 46 batch loss is 3.3633391857147217\n",
      "The 47 batch loss is 3.2134578227996826\n",
      "The 48 batch loss is 3.172513246536255\n",
      "The 49 batch loss is 3.249166250228882\n",
      "The 50 batch loss is 3.5384681224823\n",
      "The 51 batch loss is 2.9895737171173096\n",
      "The 52 batch loss is 3.425769329071045\n",
      "The 53 batch loss is 3.3752832412719727\n",
      "The 54 batch loss is 3.13382887840271\n",
      "The 55 batch loss is 3.533287763595581\n",
      "The 56 batch loss is 2.9672646522521973\n",
      "The 57 batch loss is 3.4179904460906982\n",
      "The 58 batch loss is 3.4957847595214844\n",
      "The 59 batch loss is 3.426227569580078\n",
      "The 60 batch loss is 3.406035900115967\n",
      "The 61 batch loss is 3.5313918590545654\n",
      "The 62 batch loss is 3.2086193561553955\n",
      "The 63 batch loss is 3.2108571529388428\n",
      "The 64 batch loss is 3.265646457672119\n",
      "The 65 batch loss is 3.0591776371002197\n",
      "The 66 batch loss is 3.3279099464416504\n",
      "The 67 batch loss is 3.223379611968994\n",
      "The 68 batch loss is 3.19518780708313\n",
      "The 69 batch loss is 3.1328296661376953\n",
      "The 70 batch loss is 3.108621835708618\n",
      "The 71 batch loss is 3.5005106925964355\n",
      "The 72 batch loss is 3.3763813972473145\n",
      "The 73 batch loss is 3.180785655975342\n",
      "The 74 batch loss is 3.2622601985931396\n",
      "The 75 batch loss is 3.3261213302612305\n",
      "The 76 batch loss is 3.4065566062927246\n",
      "The 77 batch loss is 3.119828224182129\n",
      "The 78 batch loss is 3.4365875720977783\n",
      "The 79 batch loss is 3.387336254119873\n",
      "The 80 batch loss is 3.2056055068969727\n",
      "The 81 batch loss is 3.188408374786377\n",
      "The 82 batch loss is 3.1357336044311523\n",
      "The 83 batch loss is 3.47564435005188\n",
      "The 84 batch loss is 3.522000789642334\n",
      "The 85 batch loss is 2.8919644355773926\n",
      "The 86 batch loss is 3.2838401794433594\n",
      "The 87 batch loss is 3.321802854537964\n",
      "The 88 batch loss is 3.222132921218872\n",
      "The 89 batch loss is 3.4503543376922607\n",
      "The 90 batch loss is 3.2121939659118652\n",
      "The 91 batch loss is 3.1354868412017822\n",
      "The 92 batch loss is 3.0933444499969482\n",
      "The 93 batch loss is 3.0368409156799316\n",
      "The 94 batch loss is 3.301607847213745\n",
      "The 95 batch loss is 3.3958802223205566\n",
      "The 96 batch loss is 3.266084909439087\n",
      "The 97 batch loss is 3.1465044021606445\n",
      "The 98 batch loss is 3.235729932785034\n",
      "The 99 batch loss is 3.3759446144104004\n",
      "The 100 batch loss is 3.14259934425354\n",
      "The 101 batch loss is 3.2181179523468018\n",
      "The 102 batch loss is 3.2991855144500732\n",
      "The 103 batch loss is 3.152646541595459\n",
      "The 104 batch loss is 3.0826492309570312\n",
      "The 105 batch loss is 3.3141207695007324\n",
      "The 106 batch loss is 3.138788938522339\n",
      "The 107 batch loss is 3.260734796524048\n",
      "The 108 batch loss is 3.396385669708252\n",
      "The 109 batch loss is 2.9090447425842285\n",
      "The 110 batch loss is 3.2311792373657227\n",
      "The 111 batch loss is 3.3687663078308105\n",
      "The 112 batch loss is 3.2665700912475586\n",
      "The 113 batch loss is 3.3463516235351562\n",
      "The 114 batch loss is 3.13948917388916\n",
      "The 115 batch loss is 3.5302515029907227\n",
      "The 116 batch loss is 3.5056371688842773\n",
      "The 117 batch loss is 3.494319200515747\n",
      "The 118 batch loss is 3.4665865898132324\n",
      "The 119 batch loss is 3.3764631748199463\n",
      "The 120 batch loss is 3.6466293334960938\n",
      "The 121 batch loss is 3.2739691734313965\n",
      "The 122 batch loss is 3.1012587547302246\n",
      "The 123 batch loss is 3.526228427886963\n",
      "The 124 batch loss is 3.661176919937134\n",
      "The 125 batch loss is 3.3142786026000977\n",
      "The 126 batch loss is 3.283446788787842\n",
      "The 127 batch loss is 3.301179885864258\n",
      "The 128 batch loss is 3.138277530670166\n",
      "The 129 batch loss is 3.403219223022461\n",
      "The 130 batch loss is 3.4040751457214355\n",
      "The 131 batch loss is 3.2295644283294678\n",
      "The 132 batch loss is 3.165132761001587\n",
      "The 133 batch loss is 3.469804048538208\n",
      "The 134 batch loss is 3.2564659118652344\n",
      "The 135 batch loss is 3.1154260635375977\n",
      "The 136 batch loss is 3.1655561923980713\n",
      "The 137 batch loss is 3.1116440296173096\n",
      "The 138 batch loss is 3.291034698486328\n",
      "The 139 batch loss is 3.305716037750244\n",
      "The 140 batch loss is 3.4050323963165283\n",
      "The 141 batch loss is 3.1289048194885254\n",
      "The 142 batch loss is 3.2385880947113037\n",
      "The 143 batch loss is 3.357272148132324\n",
      "The 144 batch loss is 2.835315227508545\n",
      "The 145 batch loss is 3.209209680557251\n",
      "The 146 batch loss is 3.113805055618286\n",
      "The 147 batch loss is 3.295708417892456\n",
      "The 148 batch loss is 3.2464563846588135\n",
      "The 149 batch loss is 3.266751766204834\n",
      "The 150 batch loss is 3.275330066680908\n",
      "The 151 batch loss is 2.9998364448547363\n",
      "The 152 batch loss is 3.211559772491455\n",
      "The 153 batch loss is 3.189297676086426\n",
      "The 154 batch loss is 3.252932071685791\n",
      "The 155 batch loss is 3.2626917362213135\n",
      "The 156 batch loss is 3.2124383449554443\n",
      "The 157 batch loss is 3.239205837249756\n",
      "The 158 batch loss is 3.124850273132324\n",
      "The 159 batch loss is 3.264739751815796\n",
      "The 160 batch loss is 3.091430902481079\n",
      "The 161 batch loss is 3.315993070602417\n",
      "The 162 batch loss is 3.352149724960327\n",
      "The 163 batch loss is 3.2025041580200195\n",
      "The 164 batch loss is 3.3460121154785156\n",
      "The 165 batch loss is 3.4270124435424805\n",
      "The 166 batch loss is 3.3469038009643555\n",
      "The 167 batch loss is 3.4693243503570557\n",
      "The 168 batch loss is 3.4351561069488525\n",
      "The 169 batch loss is 3.080125093460083\n",
      "The 170 batch loss is 2.962291717529297\n",
      "The 171 batch loss is 3.481484889984131\n",
      "The 172 batch loss is 3.051997184753418\n",
      "The 173 batch loss is 3.4002609252929688\n",
      "The 174 batch loss is 3.0713722705841064\n",
      "The 175 batch loss is 3.0460002422332764\n",
      "The 176 batch loss is 3.2910027503967285\n",
      "The 177 batch loss is 3.4457013607025146\n",
      "The 178 batch loss is 2.943286895751953\n",
      "The 179 batch loss is 3.1586368083953857\n",
      "The 180 batch loss is 3.212104320526123\n",
      "The 181 batch loss is 3.26090931892395\n",
      "The 182 batch loss is 3.388732671737671\n",
      "The 183 batch loss is 3.191082715988159\n",
      "The 184 batch loss is 3.4691970348358154\n",
      "The 185 batch loss is 3.2316954135894775\n",
      "The 186 batch loss is 3.241002321243286\n",
      "The 187 batch loss is 3.171595811843872\n",
      "The 188 batch loss is 2.971092700958252\n",
      "The 189 batch loss is 3.164077043533325\n",
      "The 190 batch loss is 3.3088393211364746\n",
      "The 191 batch loss is 3.3436286449432373\n",
      "The 192 batch loss is 3.3253731727600098\n",
      "The 193 batch loss is 3.1167151927948\n",
      "The 194 batch loss is 3.3731331825256348\n",
      "The 195 batch loss is 3.439649820327759\n",
      "The 196 batch loss is 3.4429681301116943\n",
      "The 197 batch loss is 3.293663740158081\n",
      "The 198 batch loss is 3.4316272735595703\n",
      "The 199 batch loss is 3.3135037422180176\n",
      "The 200 batch loss is 3.117260694503784\n",
      "The 201 batch loss is 3.5056512355804443\n",
      "The 202 batch loss is 3.4931609630584717\n",
      "The 203 batch loss is 3.0438578128814697\n",
      "The 204 batch loss is 3.371934175491333\n",
      "The 205 batch loss is 3.050201892852783\n",
      "The 206 batch loss is 2.9769885540008545\n",
      "The 207 batch loss is 3.1661477088928223\n",
      "The 208 batch loss is 3.260209083557129\n",
      "The 209 batch loss is 3.2093212604522705\n",
      "The 210 batch loss is 3.2726480960845947\n",
      "The 211 batch loss is 2.9355742931365967\n",
      "The 212 batch loss is 3.1183111667633057\n",
      "The 213 batch loss is 3.1513636112213135\n",
      "The 214 batch loss is 3.3925724029541016\n",
      "The 215 batch loss is 3.1900558471679688\n",
      "The 216 batch loss is 3.1569178104400635\n",
      "The 217 batch loss is 2.9723732471466064\n",
      "The 218 batch loss is 3.2000017166137695\n",
      "The 219 batch loss is 3.5981762409210205\n",
      "The 220 batch loss is 3.385622978210449\n",
      "The 221 batch loss is 3.1013903617858887\n",
      "The 222 batch loss is 3.3436594009399414\n",
      "The 223 batch loss is 3.2190210819244385\n",
      "The 224 batch loss is 3.531308650970459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 225 batch loss is 3.1809329986572266\n",
      "The 226 batch loss is 2.881571054458618\n",
      "Epoch: 09 | Time: 24m 53s\n",
      "\tTrain Loss: 3.268 | Train PPL:  26.258\n",
      "\t Val. Loss: 3.972 |  Val. PPL:  53.088\n",
      "The 0 batch loss is 3.29844069480896\n",
      "The 1 batch loss is 2.8954949378967285\n",
      "The 2 batch loss is 3.1408958435058594\n",
      "The 3 batch loss is 3.2076969146728516\n",
      "The 4 batch loss is 3.1362385749816895\n",
      "The 5 batch loss is 3.12420916557312\n",
      "The 6 batch loss is 2.9977762699127197\n",
      "The 7 batch loss is 3.118506669998169\n",
      "The 8 batch loss is 3.1913790702819824\n",
      "The 9 batch loss is 3.3622617721557617\n",
      "The 10 batch loss is 3.2785580158233643\n",
      "The 11 batch loss is 3.0558714866638184\n",
      "The 12 batch loss is 2.9261820316314697\n",
      "The 13 batch loss is 3.349606990814209\n",
      "The 14 batch loss is 3.099592447280884\n",
      "The 15 batch loss is 3.0850260257720947\n",
      "The 16 batch loss is 2.88156795501709\n",
      "The 17 batch loss is 3.3520941734313965\n",
      "The 18 batch loss is 3.0549144744873047\n",
      "The 19 batch loss is 3.2173473834991455\n",
      "The 20 batch loss is 3.0755012035369873\n",
      "The 21 batch loss is 3.1837899684906006\n",
      "The 22 batch loss is 3.181293249130249\n",
      "The 23 batch loss is 3.3453330993652344\n",
      "The 24 batch loss is 3.2969119548797607\n",
      "The 25 batch loss is 3.2631397247314453\n",
      "The 26 batch loss is 3.294767141342163\n",
      "The 27 batch loss is 2.7839467525482178\n",
      "The 28 batch loss is 3.4791712760925293\n",
      "The 29 batch loss is 3.0956859588623047\n",
      "The 30 batch loss is 3.0299277305603027\n",
      "The 31 batch loss is 2.8549246788024902\n",
      "The 32 batch loss is 3.0095603466033936\n",
      "The 33 batch loss is 3.0713603496551514\n",
      "The 34 batch loss is 3.3004939556121826\n",
      "The 35 batch loss is 2.93151593208313\n",
      "The 36 batch loss is 2.9482266902923584\n",
      "The 37 batch loss is 3.0331380367279053\n",
      "The 38 batch loss is 3.1927528381347656\n",
      "The 39 batch loss is 3.095121383666992\n",
      "The 40 batch loss is 3.1631267070770264\n",
      "The 41 batch loss is 2.623767614364624\n",
      "The 42 batch loss is 3.1508400440216064\n",
      "The 43 batch loss is 3.161328077316284\n",
      "The 44 batch loss is 2.8517329692840576\n",
      "The 45 batch loss is 3.141871213912964\n",
      "The 46 batch loss is 3.004657506942749\n",
      "The 47 batch loss is 3.352470636367798\n",
      "The 48 batch loss is 3.1353986263275146\n",
      "The 49 batch loss is 3.050610065460205\n",
      "The 50 batch loss is 3.0497145652770996\n",
      "The 51 batch loss is 3.1683509349823\n",
      "The 52 batch loss is 3.1295111179351807\n",
      "The 53 batch loss is 2.958148717880249\n",
      "The 54 batch loss is 3.3677217960357666\n",
      "The 55 batch loss is 3.2861416339874268\n",
      "The 56 batch loss is 3.121927499771118\n",
      "The 57 batch loss is 3.041452169418335\n",
      "The 58 batch loss is 3.4041121006011963\n",
      "The 59 batch loss is 2.877333164215088\n",
      "The 60 batch loss is 3.318000078201294\n",
      "The 61 batch loss is 3.3747146129608154\n",
      "The 62 batch loss is 3.066575288772583\n",
      "The 63 batch loss is 3.2090189456939697\n",
      "The 64 batch loss is 3.474208116531372\n",
      "The 65 batch loss is 3.5307343006134033\n",
      "The 66 batch loss is 3.2468061447143555\n",
      "The 67 batch loss is 2.950178384780884\n",
      "The 68 batch loss is 3.2889301776885986\n",
      "The 69 batch loss is 3.3285536766052246\n",
      "The 70 batch loss is 2.915269613265991\n",
      "The 71 batch loss is 3.0305280685424805\n",
      "The 72 batch loss is 2.976727247238159\n",
      "The 73 batch loss is 2.992481231689453\n",
      "The 74 batch loss is 2.9752302169799805\n",
      "The 75 batch loss is 3.305527448654175\n",
      "The 76 batch loss is 2.9991087913513184\n",
      "The 77 batch loss is 3.129730701446533\n",
      "The 78 batch loss is 3.211663246154785\n",
      "The 79 batch loss is 3.109973430633545\n",
      "The 80 batch loss is 2.8924784660339355\n",
      "The 81 batch loss is 3.0412771701812744\n",
      "The 82 batch loss is 2.979860544204712\n",
      "The 83 batch loss is 3.1356613636016846\n",
      "The 84 batch loss is 3.469416856765747\n",
      "The 85 batch loss is 3.233297824859619\n",
      "The 86 batch loss is 3.6423299312591553\n",
      "The 87 batch loss is 3.2398524284362793\n",
      "The 88 batch loss is 3.4256083965301514\n",
      "The 89 batch loss is 3.131549835205078\n",
      "The 90 batch loss is 2.995133876800537\n",
      "The 91 batch loss is 2.917273759841919\n",
      "The 92 batch loss is 2.9745569229125977\n",
      "The 93 batch loss is 3.1562998294830322\n",
      "The 94 batch loss is 2.910327434539795\n",
      "The 95 batch loss is 3.0491063594818115\n",
      "The 96 batch loss is 3.137324333190918\n",
      "The 97 batch loss is 3.075378894805908\n",
      "The 98 batch loss is 2.978771686553955\n",
      "The 99 batch loss is 3.0547993183135986\n",
      "The 100 batch loss is 2.9496915340423584\n",
      "The 101 batch loss is 3.2255022525787354\n",
      "The 102 batch loss is 3.0906593799591064\n",
      "The 103 batch loss is 2.7385683059692383\n",
      "The 104 batch loss is 3.4056396484375\n",
      "The 105 batch loss is 3.139317274093628\n",
      "The 106 batch loss is 3.061555862426758\n",
      "The 107 batch loss is 2.8674957752227783\n",
      "The 108 batch loss is 3.299818515777588\n",
      "The 109 batch loss is 3.0168232917785645\n",
      "The 110 batch loss is 3.065253257751465\n",
      "The 111 batch loss is 3.3598310947418213\n",
      "The 112 batch loss is 3.4103844165802\n",
      "The 113 batch loss is 3.0473649501800537\n",
      "The 114 batch loss is 3.1391849517822266\n",
      "The 115 batch loss is 2.891223192214966\n",
      "The 116 batch loss is 3.153811454772949\n",
      "The 117 batch loss is 3.0908315181732178\n",
      "The 118 batch loss is 3.068854331970215\n",
      "The 119 batch loss is 3.190239906311035\n",
      "The 120 batch loss is 3.115330934524536\n",
      "The 121 batch loss is 2.905102491378784\n",
      "The 122 batch loss is 3.049095392227173\n",
      "The 123 batch loss is 3.1488265991210938\n",
      "The 124 batch loss is 3.3425662517547607\n",
      "The 125 batch loss is 3.0140018463134766\n",
      "The 126 batch loss is 3.193402051925659\n",
      "The 127 batch loss is 3.0953471660614014\n",
      "The 128 batch loss is 3.1744511127471924\n",
      "The 129 batch loss is 2.9668612480163574\n",
      "The 130 batch loss is 3.1879982948303223\n",
      "The 131 batch loss is 3.040053606033325\n",
      "The 132 batch loss is 3.3395071029663086\n",
      "The 133 batch loss is 3.1579222679138184\n",
      "The 134 batch loss is 3.180914878845215\n",
      "The 135 batch loss is 3.1922130584716797\n",
      "The 136 batch loss is 3.2182366847991943\n",
      "The 137 batch loss is 3.189965009689331\n",
      "The 138 batch loss is 3.099903106689453\n",
      "The 139 batch loss is 3.263906478881836\n",
      "The 140 batch loss is 3.2821264266967773\n",
      "The 141 batch loss is 3.06184458732605\n",
      "The 142 batch loss is 3.0172014236450195\n",
      "The 143 batch loss is 2.98085618019104\n",
      "The 144 batch loss is 3.6020171642303467\n",
      "The 145 batch loss is 3.0214712619781494\n",
      "The 146 batch loss is 3.2438337802886963\n",
      "The 147 batch loss is 3.231513023376465\n",
      "The 148 batch loss is 3.2383368015289307\n",
      "The 149 batch loss is 3.101942777633667\n",
      "The 150 batch loss is 3.251061201095581\n",
      "The 151 batch loss is 3.2287702560424805\n",
      "The 152 batch loss is 2.830343723297119\n",
      "The 153 batch loss is 2.946903944015503\n",
      "The 154 batch loss is 2.975356340408325\n",
      "The 155 batch loss is 2.890913248062134\n",
      "The 156 batch loss is 2.9366679191589355\n",
      "The 157 batch loss is 2.8771235942840576\n",
      "The 158 batch loss is 2.9065985679626465\n",
      "The 159 batch loss is 3.197742462158203\n",
      "The 160 batch loss is 3.1795284748077393\n",
      "The 161 batch loss is 2.8344149589538574\n",
      "The 162 batch loss is 3.307384490966797\n",
      "The 163 batch loss is 2.946798324584961\n",
      "The 164 batch loss is 3.5378310680389404\n",
      "The 165 batch loss is 3.3072967529296875\n",
      "The 166 batch loss is 2.920858144760132\n",
      "The 167 batch loss is 3.419492721557617\n",
      "The 168 batch loss is 3.2099416255950928\n",
      "The 169 batch loss is 3.1322712898254395\n",
      "The 170 batch loss is 3.21555495262146\n",
      "The 171 batch loss is 3.111234188079834\n",
      "The 172 batch loss is 3.1366701126098633\n",
      "The 173 batch loss is 3.4110000133514404\n",
      "The 174 batch loss is 2.9851932525634766\n",
      "The 175 batch loss is 3.20528507232666\n",
      "The 176 batch loss is 3.0676751136779785\n",
      "The 177 batch loss is 2.8893239498138428\n",
      "The 178 batch loss is 3.1720402240753174\n",
      "The 179 batch loss is 2.9678075313568115\n",
      "The 180 batch loss is 3.0585763454437256\n",
      "The 181 batch loss is 3.0351455211639404\n",
      "The 182 batch loss is 3.013667345046997\n",
      "The 183 batch loss is 3.020866870880127\n",
      "The 184 batch loss is 3.281139373779297\n",
      "The 185 batch loss is 3.3959219455718994\n",
      "The 186 batch loss is 3.0837111473083496\n",
      "The 187 batch loss is 3.0538735389709473\n",
      "The 188 batch loss is 3.291473865509033\n",
      "The 189 batch loss is 3.2845358848571777\n",
      "The 190 batch loss is 3.2975924015045166\n",
      "The 191 batch loss is 3.287926435470581\n",
      "The 192 batch loss is 3.2235729694366455\n",
      "The 193 batch loss is 2.8571083545684814\n",
      "The 194 batch loss is 3.3551185131073\n",
      "The 195 batch loss is 3.076186418533325\n",
      "The 196 batch loss is 2.9365482330322266\n",
      "The 197 batch loss is 3.1615889072418213\n",
      "The 198 batch loss is 3.2070703506469727\n",
      "The 199 batch loss is 2.984042167663574\n",
      "The 200 batch loss is 2.9453959465026855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 201 batch loss is 3.1332168579101562\n",
      "The 202 batch loss is 3.2177696228027344\n",
      "The 203 batch loss is 3.1426827907562256\n",
      "The 204 batch loss is 3.1447064876556396\n",
      "The 205 batch loss is 3.222234010696411\n",
      "The 206 batch loss is 2.9496994018554688\n",
      "The 207 batch loss is 3.3220250606536865\n",
      "The 208 batch loss is 3.1690926551818848\n",
      "The 209 batch loss is 3.1986851692199707\n",
      "The 210 batch loss is 3.609957456588745\n",
      "The 211 batch loss is 3.264378309249878\n",
      "The 212 batch loss is 3.2297916412353516\n",
      "The 213 batch loss is 2.9110982418060303\n",
      "The 214 batch loss is 3.213965654373169\n",
      "The 215 batch loss is 3.351102352142334\n",
      "The 216 batch loss is 2.8525805473327637\n",
      "The 217 batch loss is 3.1235055923461914\n",
      "The 218 batch loss is 3.174438238143921\n",
      "The 219 batch loss is 3.238590717315674\n",
      "The 220 batch loss is 2.8850996494293213\n",
      "The 221 batch loss is 3.688338279724121\n",
      "The 222 batch loss is 3.265326499938965\n",
      "The 223 batch loss is 2.8652491569519043\n",
      "The 224 batch loss is 3.0527756214141846\n",
      "The 225 batch loss is 3.030766725540161\n",
      "The 226 batch loss is 2.9963619709014893\n",
      "Epoch: 10 | Time: 24m 50s\n",
      "\tTrain Loss: 3.133 | Train PPL:  22.936\n",
      "\t Val. Loss: 3.845 |  Val. PPL:  46.770\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载模型，测试集测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 3.825 | Test PPL:  45.832 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
